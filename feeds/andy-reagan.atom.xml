<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>andy reagan - Andy Reagan</title><link href="/" rel="alternate"></link><link href="/feeds/andy-reagan.atom.xml" rel="self"></link><id>/</id><updated>2019-11-01T00:00:00+01:00</updated><entry><title>2019 NYC Marathon</title><link href="/2019/11/01/2019-nyc-marathon/" rel="alternate"></link><published>2019-11-01T00:00:00+01:00</published><updated>2019-11-01T00:00:00+01:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2019-11-01:/2019/11/01/2019-nyc-marathon/</id><summary type="html">&lt;p&gt;Long story stort:
I didn't run.&lt;/p&gt;
&lt;p&gt;In the spirit of sharing both successes and failures,
I overcommitted personally
and opted to skip the run.
Instead,
I drove past NYC
and spent the afternoon with my wife and kiddos.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Long story stort:
I didn't run.&lt;/p&gt;
&lt;p&gt;In the spirit of sharing both successes and failures,
I overcommitted personally
and opted to skip the run.
Instead,
I drove past NYC
and spent the afternoon with my wife and kiddos.&lt;/p&gt;</content></entry><entry><title>Presque Isle Marathon</title><link href="/2019/09/08/presque-isle-marathon/" rel="alternate"></link><published>2019-09-08T00:00:00+02:00</published><updated>2019-09-08T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2019-09-08:/2019/09/08/presque-isle-marathon/</id><summary type="html">&lt;p&gt;PR Baby! Boston bound for 2020.&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/2691066468/embed/2e0692df9dc12cf1f22e8c6c538c26950a48ce70'&gt;&lt;/iframe&gt;</summary><content type="html">&lt;p&gt;PR Baby! Boston bound for 2020.&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/2691066468/embed/2e0692df9dc12cf1f22e8c6c538c26950a48ce70'&gt;&lt;/iframe&gt;</content></entry><entry><title>Mexico with the boys</title><link href="/2019/08/22/mexico-with-the-boys/" rel="alternate"></link><published>2019-08-22T00:00:00+02:00</published><updated>2019-08-22T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2019-08-22:/2019/08/22/mexico-with-the-boys/</id><summary type="html">&lt;p&gt;Hard to believe,
but I snuck out of family responsibilities
and went to Mexico for 5 days for a bachelor party!&lt;/p&gt;
&lt;p&gt;A great time was had.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Hard to believe,
but I snuck out of family responsibilities
and went to Mexico for 5 days for a bachelor party!&lt;/p&gt;
&lt;p&gt;A great time was had.&lt;/p&gt;</content></entry><entry><title>Exploring the science behind the Yasso 800</title><link href="/2019/08/17/exploring-the-science-behind-the-yasso-800/" rel="alternate"></link><published>2019-08-17T00:00:00+02:00</published><updated>2019-08-17T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2019-08-17:/2019/08/17/exploring-the-science-behind-the-yasso-800/</id><summary type="html">&lt;p&gt;If you haven't heard of them,
&lt;a href="https://www.runnersworld.com/race-training/the-real-history-of-the-yasso-800s"&gt;Yasso 800s&lt;/a&gt; comprise a infamous running workout that touts itself to predict your marathon time.
The name was coined by Amby Burfoot,
paying homage to Runner's World editor Bart Yasso.&lt;/p&gt;
&lt;p&gt;While even Yasso himself has professed &lt;a href="https://www.runnersworld.com/runners-stories/a20810642/yasso-800s-make-no-sense/"&gt;he had no idea&lt;/a&gt; why the math worked …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you haven't heard of them,
&lt;a href="https://www.runnersworld.com/race-training/the-real-history-of-the-yasso-800s"&gt;Yasso 800s&lt;/a&gt; comprise a infamous running workout that touts itself to predict your marathon time.
The name was coined by Amby Burfoot,
paying homage to Runner's World editor Bart Yasso.&lt;/p&gt;
&lt;p&gt;While even Yasso himself has professed &lt;a href="https://www.runnersworld.com/runners-stories/a20810642/yasso-800s-make-no-sense/"&gt;he had no idea&lt;/a&gt; why the math worked,
many others have remained stumped.
No more!
First,
let's define the workout:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run 10 800-meter intervals, with 2–3 minutes rest.&lt;/li&gt;
&lt;li&gt;Take you average time for the intervals, replace the minutes with hours, the seconds with minutes, and that’s your marathon time!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you ran the repeats averaging 3:35 (minutes:seconds),
that means you’re ready to run 3:35 (hours:minutes) in the marathon.
Simple.&lt;/p&gt;
&lt;p&gt;The trick here is also simple:
you’re multiplying your time by 60,
but the distance by only 52.4 (.5 miles to 26.2 miles),
so the pace is slower.
Note that you have done 10 repeats so the pace of the repeats themselves is closer to what you can run for 5 miles.
Let your time for the repeats by t₁,
and the time for the marathon t₂.
Yasso says that t₂ = 60 ⋅ t₁.
So in the same amount of time,
you cover less distance.
In doing so,
we’re saying you pace will only be 52.4/60 = 87.3% of your pace on intervals,
so you’re running 87.3% of the pace you can for about 5 miles.&lt;/p&gt;
&lt;p&gt;That helped, right? Almost! Thanks to work done by &lt;a href="http://downloads.hindawi.com/journals/bmri/2018/8203062.pdf"&gt;academic&lt;/a&gt; &lt;a href="https://bmcsportsscimedrehabil.biomedcentral.com/articles/10.1186/s13102-016-0052-y"&gt;studies&lt;/a&gt;, we know that the pace/distance curve has another neat, mathematical property: it follows a power law. The power law curve is so-named because it resembles y = axᵇ , where x is raised to the b-th power.&lt;/p&gt;
&lt;h2&gt;Let’s check this formula ourselves&lt;/h2&gt;
&lt;p&gt;To understand this relationship, take a look at the record times at distances ranging from the 100 meters through the marathon. While no single athlete would be ready for events from the 100m through the marathon, we can consider the best times in event as a measure of what is possible. World record performances are here on &lt;a href="https://en.wikipedia.org/wiki/List_of_world_records_in_athletics#Men"&gt;Wikipedia&lt;/a&gt;, and we convert to CSV using &lt;a href="https://wikitable2csv.ggor.de/"&gt;this webpage&lt;/a&gt;. We end up with data like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-08-17-yasso/1.png" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;To understand the data, we’ll start by making a plot of pace vs distance.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-08-17-yasso/2.png" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;While that curve doesn’t look special to most, to those familiar with &lt;a href="vermontcomplexsystems.org"&gt;Complex Systems&lt;/a&gt; see that it has the makings of a power law. As we mentioned, analysis has already been done to show that the a &lt;a href="wikipedia.org/power_law"&gt;power law&lt;/a&gt; model will fit this data, and let’s take a look for ourselves. The first thing we need to is plot this data in log-log space.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-08-17-yasso/3.png" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Hey, it (sortof) looks like a straight line now! Now we can fit a simple regression line to the data, and that will give us the coefficients on the power law model. The regression in log space gives us a fit show on the next plot. I’ve excluded the points in red that are slower than longer events.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-08-17-yasso/4.png" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;We found coefficients for pace = 17.85 ⋅ distance^(−0.1144).&lt;/p&gt;
&lt;h2&gt;Now let’s check Yasso’s math.&lt;/h2&gt;
&lt;p&gt;Earlier, we figured out that Yasso said marathon pace is 87% of your 800m pace (that you can repeat 10 times). As it turned out, that was the same as just multiplying the time by 60. For ourselves, we found a formula that gives us pace = 17.85 ⋅ distance^(−0.1144), based on the data, and that gives us a ratio of 82% from 5 miles to the marathon. So Yasso 800s are going to largely overestimate your pace for the marathon, if you’re running world record pace for both the 5mi and the marathon. If we compare our model to &lt;a href="http://downloads.hindawi.com/journals/bmri/2018/8203062.pdf"&gt;those fit on individual athletes&lt;/a&gt;, we do see that ours is “steeper”, such that it might be closer to 87% than 82%.&lt;/p&gt;
&lt;p&gt;For fun, we can play around with these formulas to make new workouts! Next time you’re curious about your marathon potential,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Go run 4 x mile.&lt;/li&gt;
&lt;li&gt;Take your mile split in seconds, multiply by 0.5, and that’s your marathon time in minutes!†&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I’ll let you decide what to call this workout.&lt;/p&gt;
&lt;hr&gt;

&lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;p&gt;† Based on the formula we found, it should really be .55!&lt;/p&gt;</content></entry><entry><title>Developing Python on Vertica</title><link href="/2019/07/05/developing-python-on-vertica/" rel="alternate"></link><published>2019-07-05T00:00:00+02:00</published><updated>2019-07-05T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2019-07-05:/2019/07/05/developing-python-on-vertica/</id><summary type="html">&lt;p&gt;Vertica is a very powerful analytics database,
and we can easily extend functionality now by building in Python functions.
This is great and all,
so here I'll focus on setting up a development environment for building a simple UDx.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/ExtendingVertica/UDx/DevEnvironment.htm"&gt;documentation from Vertica&lt;/a&gt; is not super specific, so this may …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Vertica is a very powerful analytics database,
and we can easily extend functionality now by building in Python functions.
This is great and all,
so here I'll focus on setting up a development environment for building a simple UDx.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://www.vertica.com/docs/9.2.x/HTML/Content/Authoring/ExtendingVertica/UDx/DevEnvironment.htm"&gt;documentation from Vertica&lt;/a&gt; is not super specific, so this may help!&lt;/p&gt;
&lt;p&gt;For the local setup, we'll be using https://github.com/jbfavre/docker-vertica/.
I'm going to focus on the CentOS version, since it is most similar to Amazon's RHEL images.
To get started (on OSX, you need the Docker client first):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker pull jbfavre/vertica:9.2.0-7_centos-7
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Also take a second to get &lt;a href="https://www.python.org/downloads/release/python-351/"&gt;Python 3.5.1 here&lt;/a&gt;.
Those are the two steps that require the most bandwidth!&lt;/p&gt;
&lt;p&gt;Now, run that thing:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker run -p 5433:5433 jbfavre/vertica:9.2.0-7_centos-7
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Grab the name of the running container from &lt;code&gt;docker ps&lt;/code&gt; command, and set it to &lt;code&gt;$DOCKER_NAME&lt;/code&gt;, and copy in our Python source:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;DOCKER_NAME=suspicious_chatelet
docker cp Downloads/Python-3.5.1.tgz $DOCKER_NAME:/home/dbadmin/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Phew! We're done with the parts that are specific to a local setup.
If you're working on a server somewhere, copy up the &lt;code&gt;Python-3.5.1.tgz&lt;/code&gt;
and put it at &lt;code&gt;/home/dbadmin/Python-3.5.1.tgz&lt;/code&gt;.
For good measure (i.e., if you copied with the root user) make sure it's owned by &lt;code&gt;dbadmin&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;chown dbadmin /home/dbadmin/Python-3.5.1.tgz
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;As root, do&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;yum install openssl-devel bzip2-devel expat-devel gdbm-devel readline-devel sqlite-devel
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Grab a coffee.&lt;/p&gt;
&lt;p&gt;Switch to &lt;code&gt;dbadmin&lt;/code&gt; user and do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cd /home/dbadmin/Python-3.5.1
./configure
make
make install
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Probably another coffee for that one!&lt;/p&gt;
&lt;p&gt;If you run the above as &lt;code&gt;dbadmin&lt;/code&gt;, which you should, skip the &lt;code&gt;make install&lt;/code&gt; line.
Or only do that last line as &lt;code&gt;root&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Maybe still as root, though again this should be fine (and preferable) as dbadmin user in the &lt;code&gt;/home/dbadmin&lt;/code&gt; directory. If you used root for anything above other than &lt;code&gt;make install&lt;/code&gt;, you might need to keep &lt;code&gt;root&lt;/code&gt; user active. Here goes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Python-3.5.1/python -m venv pyenv
pyenv/bin/pip install -U pip
pyenv/bin/pip install requirements.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you're within a firewall, you might need &lt;code&gt;--index-url=[your company's artifactory]&lt;/code&gt; for the pip install lines.
What is the &lt;code&gt;requirements.txt&lt;/code&gt; line? All of the packages that your function needs,
just like the requirements file from any python package.&lt;/p&gt;
&lt;p&gt;Finally you should be able to execute the SQL needed to build your function!
Put your code in &lt;code&gt;myfunction.py&lt;/code&gt; and then you can run &lt;code&gt;vsql&lt;/code&gt; as dbadmin, and do:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;\set libfile &amp;#39;\&amp;#39;&amp;#39;`pwd`&amp;#39;/myfunction.py\&amp;#39;&amp;#39;
DROP LIBRARY mylib CASCADE;

CREATE LIBRARY mylib AS :libfile DEPENDS &amp;#39;/home/dbadmin/pyenv/lib/python3.5/site-packages/&amp;#39; LANGUAGE &amp;#39;Python&amp;#39;;
CREATE FUNCTION myfunction AS NAME &amp;#39;myfunction_factory&amp;#39; LIBRARY mylib;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you run into permissions issues having done all of the Python stuff as &lt;code&gt;root&lt;/code&gt;,
you can get the function to build by opening up the virtual environment permissions
using these commands (as &lt;code&gt;root&lt;/code&gt; in &lt;code&gt;/home/dbadmin&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;find pyenv -type f -exec chmod 666 {} \;
find . -type d -exec chmod 777 {} \;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Cheers!
I could build the python3 and package that into a Docker based off the original,
but that would be less fun for you.&lt;/p&gt;</content></entry><entry><title>Wisdom</title><link href="/2019/07/01/wisdom/" rel="alternate"></link><published>2019-07-01T00:00:00+02:00</published><updated>2019-07-01T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2019-07-01:/2019/07/01/wisdom/</id><summary type="html">&lt;p&gt;This month I turn 30 years old and what a wild ride it has been!
I am very grateful for everyone who has been with me for my journey,
and am hopeful of many adventures to come.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This month I turn 30 years old and what a wild ride it has been!
I am very grateful for everyone who has been with me for my journey,
and am hopeful of many adventures to come.&lt;/p&gt;</content></entry><entry><title>Welcome Abel Francis</title><link href="/2019/04/22/welcome-abel-francis/" rel="alternate"></link><published>2019-04-22T00:00:00+02:00</published><updated>2019-04-22T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2019-04-22:/2019/04/22/welcome-abel-francis/</id><summary type="html">&lt;p&gt;Abel Francis was born in the afternoon of April 22nd, 2019, at nearly nine pounds!&lt;/p&gt;
&lt;p&gt;Mom and baby are happy and healthy, here's the little man shortly after birth:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-22-welcome-abel/20190422_170854.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Olson loves his little brother, seen here at a little over a month old:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-22-welcome-abel/20190527_191950.jpg" class="img-responsive"&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Abel Francis was born in the afternoon of April 22nd, 2019, at nearly nine pounds!&lt;/p&gt;
&lt;p&gt;Mom and baby are happy and healthy, here's the little man shortly after birth:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-22-welcome-abel/20190422_170854.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Olson loves his little brother, seen here at a little over a month old:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-22-welcome-abel/20190527_191950.jpg" class="img-responsive"&gt;&lt;/p&gt;</content></entry><entry><title>Boston Marathon 2019</title><link href="/2019/04/15/boston-marathon-2019/" rel="alternate"></link><published>2019-04-15T00:00:00+02:00</published><updated>2019-04-15T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2019-04-15:/2019/04/15/boston-marathon-2019/</id><summary type="html">&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/2292189157/embed/4cb5a296f8edaa8b8c76b4bc15d7d91084a8c8fb'&gt;&lt;/iframe&gt;

&lt;p&gt;What a day! Congrats to all the finishers.
I count myself very grateful to be at the start line of this one,
with my wife at 39 weeks pregnant.
Didn't have the run that I had hoped out there,
wrote some checks that my legs couldn't cash with a first …&lt;/p&gt;</summary><content type="html">&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/2292189157/embed/4cb5a296f8edaa8b8c76b4bc15d7d91084a8c8fb'&gt;&lt;/iframe&gt;

&lt;p&gt;What a day! Congrats to all the finishers.
I count myself very grateful to be at the start line of this one,
with my wife at 39 weeks pregnant.
Didn't have the run that I had hoped out there,
wrote some checks that my legs couldn't cash with a first half closer to 1:27
with a lot of small surges.&lt;/p&gt;
&lt;p&gt;Instagram:&lt;/p&gt;
&lt;blockquote class="instagram-media" data-instgrm-captioned data-instgrm-permalink="https://www.instagram.com/p/BwSe6RFALy3/" data-instgrm-version="12" style=" background:#FFF; border:0; border-radius:3px; box-shadow:0 0 1px 0 rgba(0,0,0,0.5),0 1px 10px 0 rgba(0,0,0,0.15); margin: 1px; max-width:540px; min-width:326px; padding:0; width:99.375%; width:-webkit-calc(100% - 2px); width:calc(100% - 2px);"&gt;&lt;div style="padding:16px;"&gt; &lt;a href="https://www.instagram.com/p/BwSe6RFALy3/" style=" background:#FFFFFF; line-height:0; padding:0 0; text-align:center; text-decoration:none; width:100%;" target="_blank"&gt; &lt;div style=" display: flex; flex-direction: row; align-items: center;"&gt; &lt;div style="background-color: #F4F4F4; border-radius: 50%; flex-grow: 0; height: 40px; margin-right: 14px; width: 40px;"&gt;&lt;/div&gt; &lt;div style="display: flex; flex-direction: column; flex-grow: 1; justify-content: center;"&gt; &lt;div style=" background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; margin-bottom: 6px; width: 100px;"&gt;&lt;/div&gt; &lt;div style=" background-color: #F4F4F4; border-radius: 4px; flex-grow: 0; height: 14px; width: 60px;"&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style="padding: 19% 0;"&gt;&lt;/div&gt; &lt;div style="display:block; height:50px; margin:0 auto 12px; width:50px;"&gt;&lt;svg width="50px" height="50px" viewBox="0 0 60 60" version="1.1" xmlns="https://www.w3.org/2000/svg" xmlns:xlink="https://www.w3.org/1999/xlink"&gt;&lt;g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"&gt;&lt;g transform="translate(-511.000000, -20.000000)" fill="#000000"&gt;&lt;g&gt;&lt;path d="M556.869,30.41 C554.814,30.41 553.148,32.076 553.148,34.131 C553.148,36.186 554.814,37.852 556.869,37.852 C558.924,37.852 560.59,36.186 560.59,34.131 C560.59,32.076 558.924,30.41 556.869,30.41 M541,60.657 C535.114,60.657 530.342,55.887 530.342,50 C530.342,44.114 535.114,39.342 541,39.342 C546.887,39.342 551.658,44.114 551.658,50 C551.658,55.887 546.887,60.657 541,60.657 M541,33.886 C532.1,33.886 524.886,41.1 524.886,50 C524.886,58.899 532.1,66.113 541,66.113 C549.9,66.113 557.115,58.899 557.115,50 C557.115,41.1 549.9,33.886 541,33.886 M565.378,62.101 C565.244,65.022 564.756,66.606 564.346,67.663 C563.803,69.06 563.154,70.057 562.106,71.106 C561.058,72.155 560.06,72.803 558.662,73.347 C557.607,73.757 556.021,74.244 553.102,74.378 C549.944,74.521 548.997,74.552 541,74.552 C533.003,74.552 532.056,74.521 528.898,74.378 C525.979,74.244 524.393,73.757 523.338,73.347 C521.94,72.803 520.942,72.155 519.894,71.106 C518.846,70.057 518.197,69.06 517.654,67.663 C517.244,66.606 516.755,65.022 516.623,62.101 C516.479,58.943 516.448,57.996 516.448,50 C516.448,42.003 516.479,41.056 516.623,37.899 C516.755,34.978 517.244,33.391 517.654,32.338 C518.197,30.938 518.846,29.942 519.894,28.894 C520.942,27.846 521.94,27.196 523.338,26.654 C524.393,26.244 525.979,25.756 528.898,25.623 C532.057,25.479 533.004,25.448 541,25.448 C548.997,25.448 549.943,25.479 553.102,25.623 C556.021,25.756 557.607,26.244 558.662,26.654 C560.06,27.196 561.058,27.846 562.106,28.894 C563.154,29.942 563.803,30.938 564.346,32.338 C564.756,33.391 565.244,34.978 565.378,37.899 C565.522,41.056 565.552,42.003 565.552,50 C565.552,57.996 565.522,58.943 565.378,62.101 M570.82,37.631 C570.674,34.438 570.167,32.258 569.425,30.349 C568.659,28.377 567.633,26.702 565.965,25.035 C564.297,23.368 562.623,22.342 560.652,21.575 C558.743,20.834 556.562,20.326 553.369,20.18 C550.169,20.033 549.148,20 541,20 C532.853,20 531.831,20.033 528.631,20.18 C525.438,20.326 523.257,20.834 521.349,21.575 C519.376,22.342 517.703,23.368 516.035,25.035 C514.368,26.702 513.342,28.377 512.574,30.349 C511.834,32.258 511.326,34.438 511.181,37.631 C511.035,40.831 511,41.851 511,50 C511,58.147 511.035,59.17 511.181,62.369 C511.326,65.562 511.834,67.743 512.574,69.651 C513.342,71.625 514.368,73.296 516.035,74.965 C517.703,76.634 519.376,77.658 521.349,78.425 C523.257,79.167 525.438,79.673 528.631,79.82 C531.831,79.965 532.853,80.001 541,80.001 C549.148,80.001 550.169,79.965 553.369,79.82 C556.562,79.673 558.743,79.167 560.652,78.425 C562.623,77.658 564.297,76.634 565.965,74.965 C567.633,73.296 568.659,71.625 569.425,69.651 C570.167,67.743 570.674,65.562 570.82,62.369 C570.966,59.17 571,58.147 571,50 C571,41.851 570.966,40.831 570.82,37.631"&gt;&lt;/path&gt;&lt;/g&gt;&lt;/g&gt;&lt;/g&gt;&lt;/svg&gt;&lt;/div&gt;&lt;div style="padding-top: 8px;"&gt; &lt;div style=" color:#3897f0; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:550; line-height:18px;"&gt; View this post on Instagram&lt;/div&gt;&lt;/div&gt;&lt;div style="padding: 12.5% 0;"&gt;&lt;/div&gt; &lt;div style="display: flex; flex-direction: row; margin-bottom: 14px; align-items: center;"&gt;&lt;div&gt; &lt;div style="background-color: #F4F4F4; border-radius: 50%; height: 12.5px; width: 12.5px; transform: translateX(0px) translateY(7px);"&gt;&lt;/div&gt; &lt;div style="background-color: #F4F4F4; height: 12.5px; transform: rotate(-45deg) translateX(3px) translateY(1px); width: 12.5px; flex-grow: 0; margin-right: 14px; margin-left: 2px;"&gt;&lt;/div&gt; &lt;div style="background-color: #F4F4F4; border-radius: 50%; height: 12.5px; width: 12.5px; transform: translateX(9px) translateY(-18px);"&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style="margin-left: 8px;"&gt; &lt;div style=" background-color: #F4F4F4; border-radius: 50%; flex-grow: 0; height: 20px; width: 20px;"&gt;&lt;/div&gt; &lt;div style=" width: 0; height: 0; border-top: 2px solid transparent; border-left: 6px solid #f4f4f4; border-bottom: 2px solid transparent; transform: translateX(16px) translateY(-4px) rotate(30deg)"&gt;&lt;/div&gt;&lt;/div&gt;&lt;div style="margin-left: auto;"&gt; &lt;div style=" width: 0px; border-top: 8px solid #F4F4F4; border-right: 8px solid transparent; transform: translateY(16px);"&gt;&lt;/div&gt; &lt;div style=" background-color: #F4F4F4; flex-grow: 0; height: 12px; width: 16px; transform: translateY(-4px);"&gt;&lt;/div&gt; &lt;div style=" width: 0; height: 0; border-top: 8px solid #F4F4F4; border-left: 8px solid transparent; transform: translateY(-4px) translateX(8px);"&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/a&gt; &lt;p style=" margin:8px 0 0 0; padding:0 4px;"&gt; &lt;a href="https://www.instagram.com/p/BwSe6RFALy3/" style=" color:#000; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px; text-decoration:none; word-wrap:break-word;" target="_blank"&gt;less than glamorous at mile 23...when my legs gave up at ~mile 20. on pace through the first hills to 18, almost to 20, but heartbreak broke me. encouraging texts for @sspis1 and cheers from @dmreagan32 kept me moving. 3:20ish finish. . right quad started to fail around 18, and everything was cramping by 22 and I was walking. A generous application of cramping cream at the med tent allowed me to mostly run the final 1.5... until my left leg completely locked 200 yards from the finish line and I came to a dead stop. The crowds were amazing as I started to hobble, eventually into a &amp;#34;jog&amp;#34; through the line. . What a day! Congrats to all the finishers! . . #boston2019 #bostonmarathon&lt;/a&gt;&lt;/p&gt; &lt;p style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; line-height:17px; margin-bottom:0; margin-top:8px; overflow:hidden; padding:8px 0 7px; text-align:center; text-overflow:ellipsis; white-space:nowrap;"&gt;A post shared by &lt;a href="https://www.instagram.com/instandyreagan/" style=" color:#c9c8cd; font-family:Arial,sans-serif; font-size:14px; font-style:normal; font-weight:normal; line-height:17px;" target="_blank"&gt; Andy Reagan&lt;/a&gt; (@instandyreagan) on &lt;time style=" font-family:Arial,sans-serif; font-size:14px; line-height:17px;" datetime="2019-04-15T19:59:36+00:00"&gt;Apr 15, 2019 at 12:59pm PDT&lt;/time&gt;&lt;/p&gt;&lt;/div&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;script async src="//www.instagram.com/embed.js"&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Race morning with Olson:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-15-boston-2019/20190415_064516.jpg" class="img-responsive" style="transform: rotate(0deg);"&gt;&lt;/p&gt;
&lt;p&gt;Drop-off point, a 2 mile walk from the start was as close as my Mom could get me.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-15-boston-2019/20190415_083851.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Colby!
&lt;img src="/images/2019-04-15-boston-2019/20190415_092744.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Sufferage. Took a selfie to text to my wife at this point.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-15-boston-2019/IMG_20190415_155935_102.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;More suffering.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-15-boston-2019/Screenshot_20190418-110426_Chrome.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Running the final mile.&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-15-boston-2019/Screenshot_20190418-110415_Chrome.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-15-boston-2019/Screenshot_20190426-203046_Chrome.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Done!&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2019-04-15-boston-2019/Screenshot_20190418-110503_Chrome.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Thanks for MarathonPhoto for the unscrupulously screengrabbed and watermarked pictures.&lt;/p&gt;</content></entry><entry><title>Scoring arbitrarily large datasets with Pandas + Sklearn</title><link href="/2019/04/10/scoring-arbitrarily-large-datasets-with-pandas-sklearn/" rel="alternate"></link><published>2019-04-10T00:00:00+02:00</published><updated>2019-04-10T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2019-04-10:/2019/04/10/scoring-arbitrarily-large-datasets-with-pandas-sklearn/</id><summary type="html">&lt;p&gt;The workhorses of data analysis and modeling in the &lt;a href="http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/"&gt;Python&lt;/a&gt; universe are undoubtedly &lt;a href="http://wesmckinney.com/blog/apache-arrow-pandas-internals/"&gt;Pandas&lt;/a&gt; and &lt;a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html"&gt;Sklearn&lt;/a&gt;.
I won't extoll their virtues here, but focus on solving one limiting problem.
One of the major limitations of these libraries is the size of data they can handle.&lt;/p&gt;
&lt;p&gt;In Pandas, the rule of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The workhorses of data analysis and modeling in the &lt;a href="http://jakevdp.github.io/blog/2014/05/09/why-python-is-slow/"&gt;Python&lt;/a&gt; universe are undoubtedly &lt;a href="http://wesmckinney.com/blog/apache-arrow-pandas-internals/"&gt;Pandas&lt;/a&gt; and &lt;a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html"&gt;Sklearn&lt;/a&gt;.
I won't extoll their virtues here, but focus on solving one limiting problem.
One of the major limitations of these libraries is the size of data they can handle.&lt;/p&gt;
&lt;p&gt;In Pandas, the rule of thumb is needing 5x-10x the memory for the size of your data.
While that's fine for small datasets, things can quickly get out of hand.
At &lt;a href="https://datascience.massmutual.com/"&gt;MassMutual Data Science&lt;/a&gt;, we encountered this problem when we took a model we had trained locally,
and attempted to apply to it to the entire customer and prospect universe.
These datasets have upwards of 300 million records!&lt;/p&gt;
&lt;p&gt;There are a couple obvious directions to take this problem:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Write this model on a scalable platform like Spark.&lt;/li&gt;
&lt;li&gt;Program the model into our database system.&lt;/li&gt;
&lt;li&gt;Just get a bigger machine.&lt;/li&gt;
&lt;li&gt;Break the dataset into small pieces and apply our code.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For no particularly good reason, we had taken approach (4).
Of course, option (1) is a lot of work, if possible (a UDF for the model would always be possible).
Spark's dataframe API is quite a bit different than Pandas, so this would compromise a full rewrite.
Option (2) is only easy if we dealing with a regression model (and again, a rewrite of the data transform steps).
Option (3) seems fine, but it does have limits (and is expensive!).&lt;/p&gt;
&lt;p&gt;Anyway, let's see if we hack option (4) to work on a laptop.&lt;/p&gt;
&lt;p&gt;Our code looks basically like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="kn"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pickle&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;transform_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Series&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="n"&gt;do&lt;/span&gt; &lt;span class="n"&gt;stuff&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;apply_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fname&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;medians&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;clf.pkl&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;raw_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fname&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;clean_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transform_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;medians&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;apply_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clean_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;scores&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fname&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;_scored.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We can make this work by splitting our data into 10, or 100, parts to limit the memory needed to read the whole CSV.
The function is called like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python3 score_data.py data_part_1.csv
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which writes the file &lt;code&gt;data_part_1_scored.csv&lt;/code&gt;.
Roughly, we expect that memory and computation time will scale linearly with the size of the data.
If we need to score 10 billion rows, we need to split our data many thousand times,
and this approach becomes impractical.
(How big can pieces be for given memory footprint? etc).&lt;/p&gt;
&lt;p&gt;The fix here is to only ever read &lt;code&gt;N&lt;/code&gt; rows of data at a time.
Transform and score these &lt;code&gt;N&lt;/code&gt;, write them out, and then read the next batch.
We can accomplish this by adding a function that works like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main_streaming&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;chunk_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10000&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;medians&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;X.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="nb"&gt;open&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;clf.pkl&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pickle&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdin&lt;/span&gt;
    &lt;span class="n"&gt;score_header&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;readline&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rstrip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;,&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdout&lt;/span&gt;
    &lt;span class="nb"&gt;buffer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;buffer&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;
        &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;chunk_size&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;raw_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;score_header&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;clean_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transform_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;medians&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;apply_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clean_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getvalue&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
            &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
            &lt;span class="nb"&gt;buffer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="c1"&gt;# get the last part&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;raw_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;buffer&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                               &lt;span class="n"&gt;names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;score_header&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;clean_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;transform_data&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;medians&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;scores&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;apply_model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clean_data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;clf&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;output&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;StringIO&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;pred&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;header&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;output&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getvalue&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;g&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;close&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, we only ever accept chunksize=10000 rows into the python process.
We call our new script like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cat data_part_1.csv | python3 score_data.py &amp;gt; data_part_1_scored.csv
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;but we also don't need to worry about the size anymore (just &lt;code&gt;data.csv&lt;/code&gt; instead of &lt;code&gt;data_part_1.csv&lt;/code&gt;).
We can send an arbitrarily large file here,
and scoring time will scale linearly.&lt;/p&gt;
&lt;p&gt;As an exercise for the reader,
use multiprocessing to transform and score batches on different cores!&lt;/p&gt;
&lt;p&gt;I went ahead and tested these memory and compute time assumptions by looking at&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Wall time.&lt;/li&gt;
&lt;li&gt;Memory use.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;of each of these strategies for 100K through 500K rows of data.
Here's what we found:&lt;/p&gt;
&lt;script src="https://cdn.jsdelivr.net/npm/vega@5"&gt;&lt;/script&gt;

&lt;script src="https://cdn.jsdelivr.net/npm/vega-lite@3"&gt;&lt;/script&gt;

&lt;!-- Import vega-embed --&gt;

&lt;script src="https://cdn.jsdelivr.net/npm/vega-embed@4"&gt;&lt;/script&gt;

&lt;div id="one"&gt;&lt;/div&gt;

&lt;p&gt;As expected, time scales linearly, up to 500K rows.
For memory:&lt;/p&gt;
&lt;div id="two"&gt;&lt;/div&gt;

&lt;script type="text/javascript"&gt;
  var spec = "https://gist.githubusercontent.com/andyreagan/488042d212f6ca0c4d44ab3a8116e15a/raw/a4dc6513a09512f024fb360085c12fe1c18c2311/totaltime.vg.json";
  vegaEmbed('#one', spec).then(function(result) {
    // Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
  }).catch(console.error);
  var spec = "https://gist.githubusercontent.com/andyreagan/488042d212f6ca0c4d44ab3a8116e15a/raw/a4dc6513a09512f024fb360085c12fe1c18c2311/memory.vg.json";
  vegaEmbed('#two', spec).then(function(result) {
    // Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
  }).catch(console.error);
  var spec = "https://gist.githubusercontent.com/andyreagan/488042d212f6ca0c4d44ab3a8116e15a/raw/a4dc6513a09512f024fb360085c12fe1c18c2311/chunksize.vg.json";
  vegaEmbed('#three', spec).then(function(result) {
    // Access the Vega view instance (https://vega.github.io/vega/docs/api/view/) as result.view
  }).catch(console.error);
&lt;/script&gt;

&lt;p&gt;Again this is what we expected: memory remains constant for the streaming process.
We can easily extrapolate that a 3M line file would need ~30GB of memory!
A little bit more envelop math and we can see that it will take ~1.5 days to
score our 300M records.
The next optimization to apply is then to use all of the cores available,
and we'll stop here before re-inventing too many wheels.&lt;/p&gt;
&lt;p&gt;One more fun chart to look at how chunk size for streaming affects the time
and memory of our job.
The chunksize=1 job is still running :)&lt;/p&gt;
&lt;div id="three"&gt;&lt;/div&gt;</content></entry><entry><title>Writing LaTeX in Atom</title><link href="/2018/07/20/writing-latex-in-atom/" rel="alternate"></link><published>2018-07-20T00:00:00+02:00</published><updated>2018-07-20T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2018-07-20:/2018/07/20/writing-latex-in-atom/</id><summary type="html">&lt;p&gt;Atom is a code editor.
The defaults try to "complete" words from your writing, and don't highlight spelling.
After many months of using a code editor to write,
it's clear that I've gone backwards and I should at least be using spell checking!&lt;/p&gt;
&lt;p&gt;These two settings vastly improve the latex …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Atom is a code editor.
The defaults try to "complete" words from your writing, and don't highlight spelling.
After many months of using a code editor to write,
it's clear that I've gone backwards and I should at least be using spell checking!&lt;/p&gt;
&lt;p&gt;These two settings vastly improve the latex experience in Atom:&lt;/p&gt;
&lt;h2&gt;Add latex to spellcheck file types.&lt;/h2&gt;
&lt;p&gt;Go to the &lt;code&gt;spell-check&lt;/code&gt; package settings (it’s a core package), and add &lt;code&gt;text.tex.latex&lt;/code&gt; to the list of languages.&lt;/p&gt;
&lt;p&gt;Here's the reference that guided me: &lt;a href="https://discuss.atom.io/t/how-to-enable-spell-checking-for-another-language/4895/12"&gt;https://discuss.atom.io/t/how-to-enable-spell-checking-for-another-language/4895/12&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Turn off autocompletion in latex files.&lt;/h2&gt;
&lt;p&gt;Go to the &lt;code&gt;autocomplete-plus&lt;/code&gt; package (also a core package) and add &lt;code&gt;*.tex&lt;/code&gt; to the blacklist of file types.&lt;/p&gt;
&lt;p&gt;Here's the reference: &lt;a href="https://github.com/atom/autocomplete-atom-api/issues/19"&gt;https://github.com/atom/autocomplete-atom-api/issues/19&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;Happy TeXing!&lt;/h2&gt;</content></entry><entry><title>Tricks for coercing Pandas into parquet</title><link href="/2018/05/29/tricks-for-coercing-pandas-into-parquet/" rel="alternate"></link><published>2018-05-29T00:00:00+02:00</published><updated>2018-05-29T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2018-05-29:/2018/05/29/tricks-for-coercing-pandas-into-parquet/</id><summary type="html">&lt;p&gt;For coercing pandas date times (stored as numpy datetime):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;lt;M8[ns]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt;
    &lt;span class="c1"&gt;# https://stackoverflow.com/questions/32827169/python-reduce-precision-pandas-timestamp-dataframe&lt;/span&gt;
    &lt;span class="c1"&gt;# apply(lambda x: x.replace(microsecond=0))&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime64[s]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For coercing python datetime (here, a datetime.date, there …&lt;/p&gt;</summary><content type="html">&lt;p&gt;For coercing pandas date times (stored as numpy datetime):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtypes&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;lt;M8[ns]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)]:&lt;/span&gt;
    &lt;span class="c1"&gt;# https://stackoverflow.com/questions/32827169/python-reduce-precision-pandas-timestamp-dataframe&lt;/span&gt;
    &lt;span class="c1"&gt;# apply(lambda x: x.replace(microsecond=0))&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;astype&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;datetime64[s]&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For coercing python datetime (here, a datetime.date, there may be other options with datetime.datetime (I’ve included my failed attempts that may work there as comments)):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# df.date.values.to_timestamp()&lt;/span&gt;
&lt;span class="c1"&gt;# pd.Timestamp(df.date)&lt;/span&gt;
&lt;span class="c1"&gt;# df.date.values.astype(np.int64)&lt;/span&gt;
&lt;span class="c1"&gt;# df.loc[~df.date.isnull(),&amp;#39;date&amp;#39;].values.astype(np.int64)&lt;/span&gt;
&lt;span class="c1"&gt;# df.date.values[0].timestamp()&lt;/span&gt;
&lt;span class="c1"&gt;# pd.Timestamp(df.date.values[0],unit=&amp;#39;s&amp;#39;)&lt;/span&gt;
&lt;span class="c1"&gt;# pd.Timestamp(df.date.values[0].timestamp(),unit=&amp;#39;s&amp;#39;)&lt;/span&gt;
&lt;span class="c1"&gt;# df.date.values[0].isoformat()&lt;/span&gt;
&lt;span class="c1"&gt;# df.date.apply(lambda x: x.isoformat())&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnull&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isoformat&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For timedeltas in pandas, &lt;code&gt;timedelta64[ns]&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;timedelta_days&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;timedelta&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;days&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For mixed float and string, encoded as pandas &lt;code&gt;object&lt;/code&gt; type and &lt;code&gt;np.nan&lt;/code&gt; for nulls (this throws error):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;isfloat&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;col&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;nan&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You need these two, inefficient helpers:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;isint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;isfloat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;True&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="bp"&gt;False&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When things come in with bytes type, and you get a memoryview error, hit your dataframe with this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;stringify_df&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dtype&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;O&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;memoryview&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnull&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnull&lt;/span&gt;&lt;span class="p"&gt;(),:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tobytes&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ascii&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ignore&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnull&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnull&lt;/span&gt;&lt;span class="p"&gt;(),:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apply&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;encode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ascii&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ignore&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ascii&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;ignore&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content></entry><entry><title>Some notes about running D3 inside Jupyter</title><link href="/2018/04/27/some-notes-about-running-d3-inside-jupyter/" rel="alternate"></link><published>2018-04-27T00:00:00+02:00</published><updated>2018-04-27T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2018-04-27:/2018/04/27/some-notes-about-running-d3-inside-jupyter/</id><summary type="html">&lt;p&gt;Many visualization packages rely on using D3 in the browser, and those include:
&lt;a href="https://github.com/plotly/plotly.py/blob/master/plotly/widgets/graph_widget.py#L26"&gt;Plotly&lt;/a&gt;,
&lt;a href="https://github.com/vega/ipyvega/blob/master/vega/base.py"&gt;Vega&lt;/a&gt;,
and &lt;a href="https://github.com/mpld3/mpld3/blob/master/mpld3/_display.py"&gt;mpld3&lt;/a&gt;
(links point to the code for how these projects get JS interacting with Jupyter, using IPython’s &lt;a href="https://ipython.org/ipython-doc/3/api/generated/IPython.display.html"&gt;display module&lt;/a&gt;).
Some people have
&lt;a href="http://makeyourowntextminingtoolkit.blogspot.co.uk/2016/09/interactive-d3v4js-in-jupyter-notebook.html"&gt;no&lt;/a&gt;
&lt;a href="http://blog.thedataincubator.com/2015/08/embedding-d3-in-an-ipython-notebook/"&gt;idea&lt;/a&gt;
&lt;a href="http://www.machinalis.com/blog/embedding-interactive-charts-on-an-ipython-nb/"&gt;why&lt;/a&gt;
&lt;a href="https://multithreaded.stitchfix.com/blog/2015/12/15/d3-jupyter/"&gt;it’s&lt;/a&gt;
&lt;a href="https://www.authorea.com/users/3/articles/3904-data-driven-interactive-science-with-d3-js-plots-and-ipython-notebooks/_show_article"&gt;so&lt;/a&gt;
&lt;a href="http://stackoverflow.com/questions/41149260/d3-js-reference-error-in-jupyter-notebook-from-local-copy"&gt;hard&lt;/a&gt;,
and I’ll count …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Many visualization packages rely on using D3 in the browser, and those include:
&lt;a href="https://github.com/plotly/plotly.py/blob/master/plotly/widgets/graph_widget.py#L26"&gt;Plotly&lt;/a&gt;,
&lt;a href="https://github.com/vega/ipyvega/blob/master/vega/base.py"&gt;Vega&lt;/a&gt;,
and &lt;a href="https://github.com/mpld3/mpld3/blob/master/mpld3/_display.py"&gt;mpld3&lt;/a&gt;
(links point to the code for how these projects get JS interacting with Jupyter, using IPython’s &lt;a href="https://ipython.org/ipython-doc/3/api/generated/IPython.display.html"&gt;display module&lt;/a&gt;).
Some people have
&lt;a href="http://makeyourowntextminingtoolkit.blogspot.co.uk/2016/09/interactive-d3v4js-in-jupyter-notebook.html"&gt;no&lt;/a&gt;
&lt;a href="http://blog.thedataincubator.com/2015/08/embedding-d3-in-an-ipython-notebook/"&gt;idea&lt;/a&gt;
&lt;a href="http://www.machinalis.com/blog/embedding-interactive-charts-on-an-ipython-nb/"&gt;why&lt;/a&gt;
&lt;a href="https://multithreaded.stitchfix.com/blog/2015/12/15/d3-jupyter/"&gt;it’s&lt;/a&gt;
&lt;a href="https://www.authorea.com/users/3/articles/3904-data-driven-interactive-science-with-d3-js-plots-and-ipython-notebooks/_show_article"&gt;so&lt;/a&gt;
&lt;a href="http://stackoverflow.com/questions/41149260/d3-js-reference-error-in-jupyter-notebook-from-local-copy"&gt;hard&lt;/a&gt;,
and I’ll count myself squarely in that group
(caveat: those links are mostly random references to the topic).&lt;/p&gt;
&lt;p&gt;The part that makes things confusing is that Jupyter uses RequireJS,
which doesn’t allow you to do the simple things easily:
load d3 from a file
(via &lt;code&gt;&amp;lt;script src=...&lt;/code&gt; or by pushing the JS file as a string into the &lt;code&gt;Javascript()&lt;/code&gt; function from IPython).
I could be wrong about this,
sometimes using the script tag to load d3 works,
but it definitely doesn’t work for jquery
(it’s non-trivial for jquery: http://requirejs.org/docs/jquery.html).&lt;/p&gt;
&lt;p&gt;Of all the approaches to get D3 in the browser,
the ones that seem to do it right use RequireJS to load either a local or CDN copy of D3.
Here’s a good one:
https://github.com/ResidentMario/py_d3.
This is also exactly how MPLD3 does the load of D3, and their code is very clear.&lt;/p&gt;
&lt;hr&gt;

&lt;p&gt;Other projects:
https://github.com/jdfreder/ipython-d3networkx/
https://mail.scipy.org/pipermail/ipython-dev/2014-April/013835.html&lt;/p&gt;</content></entry><entry><title>How to groupby in Pandas with a missing group</title><link href="/2018/04/25/how-to-groupby-in-pandas-with-a-missing-group/" rel="alternate"></link><published>2018-04-25T00:00:00+02:00</published><updated>2018-04-25T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2018-04-25:/2018/04/25/how-to-groupby-in-pandas-with-a-missing-group/</id><summary type="html">&lt;p&gt;This is a note intended for my future self.
Here’s how to do it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Have a list of the values that you expect for each group.&lt;/li&gt;
&lt;li&gt;Iterate over that list, and look up the values using &lt;code&gt;.loc&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Say I want to group by months, but not all of the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a note intended for my future self.
Here’s how to do it:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Have a list of the values that you expect for each group.&lt;/li&gt;
&lt;li&gt;Iterate over that list, and look up the values using &lt;code&gt;.loc&lt;/code&gt;.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Say I want to group by months, but not all of the months will have data.
Here I’ll iterate over the known month range, and fill in as I go.
Starting with a DataFrame contain the columns BP_ID, CAL_YR, CAL_MO, and COMM_AMT_FYC (with potentially many records (or 0!) per month), we do this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;start&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2016&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;start&lt;/span&gt;
&lt;span class="n"&gt;month&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;case_count&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;fyc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;today&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;month_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CAL_YR&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;CAL_MO&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="n"&gt;case_count&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;month_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="n"&gt;fyc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;month_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;COMM_AMT_FYC&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;year&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;floor&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="o"&gt;/&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)),((&lt;/span&gt;&lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;g&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;BP_ID&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;month&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;month&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;case_count&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;case_count&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;fyc&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;fyc&lt;/span&gt;&lt;span class="p"&gt;}))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It seems inefficient, but it should get the job done.
It takes about half a second to do for the small dataframes I’m working with.
In other words, I’ll do 57K of these in 30min.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://medium.com/@andyreagan/how-to-groupby-in-pandas-with-a-missing-group-655b20d7826f"&gt;This post on Medium&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>Boston 2018</title><link href="/2018/04/16/boston-2018/" rel="alternate"></link><published>2018-04-16T00:00:00+02:00</published><updated>2018-04-16T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2018-04-16:/2018/04/16/boston-2018/</id><summary type="html">&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/1512264672/embed/5c156b1535b242d41fd467738583deb6ee37bd31'&gt;&lt;/iframe&gt;

&lt;p&gt;This was an incredible experience.&lt;/p&gt;
&lt;p&gt;The crowds were just amazing,
and seeing my son Olson at mile 6 in the rain and cold was amazing.
With the weather, Olson on my mind, and the huge crowds, parts of the race were very emotional.&lt;/p&gt;
&lt;p&gt;At about mile 11-13, I sped up …&lt;/p&gt;</summary><content type="html">&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/1512264672/embed/5c156b1535b242d41fd467738583deb6ee37bd31'&gt;&lt;/iframe&gt;

&lt;p&gt;This was an incredible experience.&lt;/p&gt;
&lt;p&gt;The crowds were just amazing,
and seeing my son Olson at mile 6 in the rain and cold was amazing.
With the weather, Olson on my mind, and the huge crowds, parts of the race were very emotional.&lt;/p&gt;
&lt;p&gt;At about mile 11-13, I sped up to stay warm at Marc's suggestion with a tight back,
he had to hold off.
I felt like I was flying for the next 10 miles,
cruised over the rollers, and was passing ~3,000 runners over this hour.
Turns out, I didn't speed up much at all,
it must have been everyone else slowing down!&lt;/p&gt;
&lt;p&gt;I really had to hang on for the finish,
and got a PR in 2:59:41!&lt;/p&gt;
&lt;h2&gt;Core body temp report&lt;/h2&gt;
&lt;p&gt;Started dropping 7:30am waiting in tents for the race,
dropped a lot more 9-10am outside in the rain pre-race,
and then I mostly warmed up during the first 6 miles which I had a thick rain jacket.&lt;/p&gt;
&lt;p&gt;Tossed that and basically maintained a good temp through the end.&lt;/p&gt;
&lt;p&gt;After the race,
soaked and shuffling through finish area,
started to get bad.
I got really cold (as did very many) in that time,
and remained cold outside for about two hours before getting a ride to a friend’s apartment to shower.
Was maybe warmed back up by 4pm (3 hours after finishing).&lt;/p&gt;
&lt;h2&gt;Recovery&lt;/h2&gt;
&lt;p&gt;Happened slowly and was aided by an IPA-fueled afterparty at home.&lt;/p&gt;</content></entry><entry><title>UVM Twitter data notes</title><link href="/2017/11/25/uvm-twitter-data-notes/" rel="alternate"></link><published>2017-11-25T00:00:00+01:00</published><updated>2017-11-25T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-11-25:/2017/11/25/uvm-twitter-data-notes/</id><summary type="html">&lt;p&gt;First, important dates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2008-09-11: We have the deca-hose from here, with a higher % of the tweets at the beginning and down to 10% now (with the total volume increasing greatly over that time). Geo is (was) roughly 1% of all tweets, first with a "coordinates" and then with the "places …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;First, important dates:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2008-09-11: We have the deca-hose from here, with a higher % of the tweets at the beginning and down to 10% now (with the total volume increasing greatly over that time). Geo is (was) roughly 1% of all tweets, first with a "coordinates" and then with the "places" field.&lt;/li&gt;
&lt;li&gt;2009-11-11: The first geo-tweet we have on the books is here: &lt;code&gt;gzip -cd zipped-raw/2009-11-11/2009-11-11-11-15.gz | less&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;2014-03-07-13-53: turned the spritzer to filter for geo tweets, pulling ~3000K per minute. Some small fraction of these didn't have a lat/lon, just had a "location”, this was about 10%. Around this time, there were ~850 geo tweets (lat/lon) per minute coming in from the gardenhose.&lt;/li&gt;
&lt;li&gt;2014-04-14: twitter upgraded their backend and we starting getting 7-10K geo tweets per minute from the spritzer.&lt;/li&gt;
&lt;li&gt;Unknown dates: There have been various outages of the geo spritzer over the past two years, but we have most of it, I think (why aren't we running 3 of them?)&lt;/li&gt;
&lt;li&gt;Unknown date: At some point, the overall volume slowed back down to 3K/minute from the geo spritzer&lt;/li&gt;
&lt;li&gt;2016-12-07: Data change from Gardenhose to GNIP decahose&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Other&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;There is a database living at UVM that Morgan built to store the geo tweets starting in 2011 in SQL...it got really big despite UVM saying they could handle it, I don’t know how to use this database&lt;ul&gt;
&lt;li&gt;Morgan: This seems correct to me. SQL was a fine solution the first year we used it for the geotagged tweets, but appears to be woefully insufficient with the current magnitude of data. One reason for this is that SQL scales vertically (i.e. with faster machines) rather than horizontally (i.e. with parallelization). Hadoop is an example that benefits from horizontal scaling, but there are others. With a beautiful tool like the VACC already in place, it seems to me that horizontal scaling is a viable option.&lt;/li&gt;
&lt;li&gt;SSH to bluemoon server&lt;/li&gt;
&lt;li&gt;`mysql -u mrfrank -p -h webdb-large1.uvm.edu -e "use MRFRANK_2; INSERT SQL COMMANDS HERE!!!!!"``&lt;/li&gt;
&lt;li&gt;you will need password, ask Morgan&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Jake build a geo database that pre-processed each tweet to place it in hierarchical boxes by country/state/county (us-analogy, these are more general) and accessed by the incredibly reliable &lt;code&gt;pullTweets.pl&lt;/code&gt; script (it's txt files for each minute stored in folders by location, all zipped up, that can be searched through without unzipping it to disk). With the decline in volume, I think that this stopped happening about a month ago, and the database isn't being updated (I could very well be wrong).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;FAQ&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Twitter says that if you pull two geo streams, they will be the same tweets (should we check? may be able to get more data by running two streams with different bounding boxes... also the speed of the scraper may matter (ruby chugger seems to be at least as fast as anything I could write in python)). how similar at the geo tweets from the spritzer and the gardenhose?&lt;ul&gt;
&lt;li&gt;Jake: I did a few tests re: volumes and multiple streams/filters a little while back and am sorry to report that (1) two (spritzer) streams running simultaneously off of the same access key produce nearly identical data (I have not acutely quantified this, but just ran a quick diff---one or two tweets were different), and (2) two disjoint hemisphere streams collectively produce the same volume as one global stream.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;The new tweets are mostly "places" tweets, and lat/lon is going away. Probably related to some business decision by twitter to integrate with foursquare (which they have), but where does this leave us?&lt;/li&gt;
&lt;li&gt;How reliable are the "places"? If there is also lat/lon in the tweet, do they generally agree?&lt;/li&gt;
&lt;li&gt;Are there tweets with both lat/lon and “places” specified?&lt;ul&gt;
&lt;li&gt;Yes, though it is a small fraction of the overall stream.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Do the tweets we’re currently collecting include the “places” specified tweets?&lt;ul&gt;
&lt;li&gt;Yes.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Am I right in assuming that the spritzer has a filter set for locations?&lt;ul&gt;
&lt;li&gt;Yes, it is the bounding box for the whole world.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Other observations&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Jake: As long as 10% of all tweets are tagged with some form of location, then application of the same filter to the garden hose will not result in reduced volume there. Presumably we could estimate the current proportion of tweets tagged with location from the spritzer, but Twitter seems to be pretty tight-lipped about their sampling methods:
https://twittercommunity.com/t/is-the-sample-streaming-api-truly-random/14942&lt;/li&gt;
&lt;li&gt;Jake, Re accuracy: My assumption is that the 'places' are reasonably accurate. It appears that they default to a detected location (once enabled), and my guess would be that most folks don't spend too much time switching locations for each tweet. However, even if this data is good, there will forever be a new cohort of data doubters who fear duplicitous behavior.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If we find that the centroids for the "places" are close to the exact geo location, we can start using those for the pullTweets database.&lt;/p&gt;
&lt;h2&gt;Analysis of "places" Tweets&lt;/h2&gt;
&lt;p&gt;Okay so here are the most recent numbers. these are the fields that we're looking for:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;tweet[&amp;quot;coordinates&amp;quot;] (ref: https://dev.twitter.com/overview/api/tweets#obj-coordinates)
tweet[&amp;quot;geo&amp;quot;] --- this was the old place for coordinates
tweet[&amp;quot;place&amp;quot;] (ref: https://dev.twitter.com/overview/api/places)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I did a parse for:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;tweets that contain any of them: "any".&lt;/li&gt;
&lt;li&gt;tweets that contain the coordinate field: "coordinates”;&lt;/li&gt;
&lt;li&gt;tweets that contain only the coordinates field: "coordinates-only”.&lt;/li&gt;
&lt;li&gt;tweets that contain only the places field: "place-only".&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Did this for both the spritzer and the main stream, here are the numbers from &lt;code&gt;wc&lt;/code&gt; (the first number is the count of Tweets):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;2016-02-15-00-00-any.gz
7815  305189 23191117

2016-02-15-00-00-any-spritzer.gz
43128 1679248 127526514

2016-02-15-00-00-coordinates.gz
1029   37231 2942433

2016-02-15-00-00-coordinates-only.gz
5     180   11608

2016-02-15-00-00-coordinates-only-spritzer.gz
30     773   69684

2016-02-15-00-00-coordinates-spritzer.gz
5655  207164 16125655

2016-02-15-00-00-place-only.gz
6786  267958 20248684

2016-02-15-00-00-place-only-spritzer.gz
37473 1472084 111400859
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Total in the spritzer is 46643 and total in the 10% feed is 398616.&lt;/p&gt;
&lt;h3&gt;Observations&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;there are 3K tweets on the spritzer that have none of these? what's up with those?&lt;/li&gt;
&lt;li&gt;VERY few (effectively none) have only the coordinates (likely folks using an out-of-date version of the Twitter app).&lt;/li&gt;
&lt;li&gt;close to a 6X boost for all categories on the spritzer.&lt;/li&gt;
&lt;li&gt;13% of the tweets with "place" also have "coordinates".&lt;/li&gt;
&lt;li&gt;we're still getting the same number of coordinates tweets from the spritzer as total geo tweets (place or coords) from the 10% stream.&lt;/li&gt;
&lt;li&gt;1.9% of tweets in the main stream have "any": we can then estimate that there are 78K total "any" tweets. we're only seeing 43K of these on the spritzer. so either we're still missing some (almost half) on the spritzer, or the 10%
over-samples the geo.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;How good is the center of bounding boxes?&lt;/h3&gt;
&lt;p&gt;Next, I'll see how close the the place is to the coordinates for that subset.&lt;/p&gt;
&lt;p&gt;So 6455 of the 6473 tweets that I tested (almost all) were inside the bounding box, with a mean distance of 52km from the centroid.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Distances" src="/images/2017-11-15-uvm-twitter-data-notes/distances.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Log10" src="/images/2017-11-15-uvm-twitter-data-notes/distances-log10.png"&gt;&lt;/p&gt;
&lt;p&gt;What the distances from the centroid look like. (note: everything in kilometers)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tell me, what does “bounding box” mean in this context
if the centroid is 52km from the tweet? Is it a county?&lt;ul&gt;
&lt;li&gt;I think that these "places" are all sorts of things. Farrell Hall is one, so is the city of Burlington, so is the United States.
Each place just comes with 4 points to describe it, and most of the points are within that bounding box.
We &lt;em&gt;could&lt;/em&gt; query more details on the place from Twitter's API, which I think is a more detailed geo fence than the bounding box.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;More next steps&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Use data for a longer timeframe (this is only 15 minutes worth).&lt;/li&gt;
&lt;li&gt;Look at the outlier tweets...&lt;/li&gt;
&lt;li&gt;Bin by the size of the bounding box (a USA box is likely to have a tweet further from the centroid), maybe small bounding boxes are most useful for the location?&lt;/li&gt;
&lt;li&gt;Cross reference the place database with the retail store locations (download their place database?).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Maybe these will be useful for specific projects, and we can test the tweet data as necessary.&lt;/p&gt;
&lt;h3&gt;Final thoughts&lt;/h3&gt;
&lt;p&gt;The statements that 13% of the tweets with "place" also have "coordinates" and 1.9% of tweets in the main stream have “any" indicate that the coordinate-tagged tweets alone now comprise a meager 0.2% of the stream in general (i.e., a drop from ~1%)?. Twitter made it harder to geo-locate with GPS when they integrated with FourSquare.&lt;/p&gt;
&lt;p&gt;The policy decision by Twitter to allow for the soft 'place' location (and its adoption by users) has watered down the data that Twitter is collecting from its users!&lt;/p&gt;</content></entry><entry><title>Run rabbit run</title><link href="/2017/11/23/run-rabbit-run/" rel="alternate"></link><published>2017-11-23T00:00:00+01:00</published><updated>2017-11-23T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-11-23:/2017/11/23/run-rabbit-run/</id><summary type="html">&lt;p&gt;A brief moment of glory:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017-11-23-trot/2017-11-23andy-reagan-bunny-suit-turkey-trot-run-race.gif" /&gt;&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/1287230193/embed/0adf742e35744a6eccfe46b5379ff22607bbf199'&gt;&lt;/iframe&gt;</summary><content type="html">&lt;p&gt;A brief moment of glory:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2017-11-23-trot/2017-11-23andy-reagan-bunny-suit-turkey-trot-run-race.gif" /&gt;&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/1287230193/embed/0adf742e35744a6eccfe46b5379ff22607bbf199'&gt;&lt;/iframe&gt;</content></entry><entry><title>Sentiment analysis methods for understanding large-scale texts: a case for using continuum-scored words and word shift graphs</title><link href="/2017/11/14/sentiment-analysis-methods-for-understanding-large-scale-texts-a-case-for-using-continuum-scored-words-and-word-shift-graphs/" rel="alternate"></link><published>2017-11-14T00:00:00+01:00</published><updated>2017-11-14T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-11-14:/2017/11/14/sentiment-analysis-methods-for-understanding-large-scale-texts-a-case-for-using-continuum-scored-words-and-word-shift-graphs/</id><summary type="html">&lt;p&gt;As a grad student trying to understand the emotional content of some unreadably large collection of texts, a typical night in grad school can often go something like this: You’re up late at night planning a new research study, thinking about trying some of this fancy sentiment-based text analysis …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As a grad student trying to understand the emotional content of some unreadably large collection of texts, a typical night in grad school can often go something like this: You’re up late at night planning a new research study, thinking about trying some of this fancy sentiment-based text analysis. You resort to your favorite search engine with the query “sentiment analysis package python.” We have all been there, except maybe with R instead of Python (the latter being my favorite).&lt;/p&gt;
&lt;p&gt;Unfortunately, despite advances in our understanding of how to perform sentiment analysis in recent years, the first method off the shelf will rarely be the best choice. Making matters worse, it can be nearly impossible to navigate the immense literature on the topic.&lt;/p&gt;
&lt;p&gt;Fortunately, we’ve figured out that there are only a few things that you need to know to get started using sentiment analysis.&lt;/p&gt;
&lt;p&gt;Using four large text corpora, we did a bake-off of sorts for dozens of tools. The methods fall into two broad categories: Supervised and unsupervised, with supervised methods relying on tagged training dataset to learn rules. Without a large training dataset, and with the desire to have a general purpose white-box tool (things can go very wrong](https://motherboard.vice.com/en_us/article/j5jmj8/google-artificial-intelligence-bias) with black box machine learning), we stick to unsupervised methods. In our recent paper in EPJ Data Science, we find that dictionary-based (unsupervised) sentiment analysis methods perform reliably on large documents (&amp;gt;10,000 words) with a few things in place:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;We show that a dictionary-based method will only perform reliably across corpora if the dictionary covers a sufficiently large enough portion of a given text’s lexicon when weighted by word usage frequency.  Summary: Make sure that your dictionary has most of the words in your documents!&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;We explicitly demonstrate that it is necessary to examine the words which contribute to sentiment ratings. Common contextual error is something we can readily identify and correct for through word shifts, but would remain hidden to naive learning methods without new training. Given our results, we emphasize that doing any type of analysis without looking at the words is a fraught activity. Here is an example word shift graph for the saddest day that we have measured on Twitter, and you can see that there are positive words (e.g. ‘prayer’) alongside the negative words making this a very sad day:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;iframe src="https://hedonometer.org/embed/main/2017-10-02/" width="590" height="550" frameborder="0" scrolling="no"&gt;&lt;/iframe&gt;

&lt;p&gt;Here is an example word shift graph for a happier-than-average day on Twitter, International Women’s Day on March 3 this year:
&lt;iframe src="https://hedonometer.org/embed/main/2017-03-08/" width="590" height="550" frameborder="0" scrolling="no"&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;ol start="3"&gt;
  &lt;li&gt;In addition, we show that a dictionary-based method will generate more meaningful word shift graphs if words are scored on a continuous scale. To interpret a sentiment score and understand the story behind the numbers, meaningful word shifts graphs are essential. The nuances in language are captured better.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now that you have some ideas about what to look for in a dictionary, we suggest you look at Table 1 in &lt;a href="https://epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-017-0121-9"&gt;our paper&lt;/a&gt;, and go measure some happiness!&lt;/p&gt;</content></entry><entry><title>Linking files from GitHub in CodePen</title><link href="/2017/11/08/linking-files-from-github-in-codepen/" rel="alternate"></link><published>2017-11-08T00:00:00+01:00</published><updated>2017-11-08T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-11-08:/2017/11/08/linking-files-from-github-in-codepen/</id><summary type="html">&lt;p&gt;In the course I teach at UC Berkeley in the MIDS program, we use CodePen to build interactive web graphics.
There are a host of reasons to use CodePen, but setting that aside for now, let's talk about how to host data files for CodePen.
CodePen lacks a way for …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In the course I teach at UC Berkeley in the MIDS program, we use CodePen to build interactive web graphics.
There are a host of reasons to use CodePen, but setting that aside for now, let's talk about how to host data files for CodePen.
CodePen lacks a way for us to store full files alongside our HTML pages, and since we're using CodePen to data vis, we need data!
Naturally, we'll load data from GitHub, where it should be (&lt;em&gt;with the exception of files over 50MB, those are too big for GitHub and probably too big to load into the browser!&lt;/em&gt;).&lt;/p&gt;
&lt;p&gt;In the course, we look at a map that myself and team of Data Scientists at MassMutual built during our Data Days for Good project.
We made a visualization of data from the Pioneer Valley Planning Commission, the PVPC, and you can see the map that we made here: &lt;a href="http://massmutual.github.io/pvpc_map"&gt;http://massmutual.github.io/pvpc_map&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To load the data files used in this map into a CodePen that can be edited, the easiest way is just click on the files in the repository, all the way down to the "raw" view of the files.
You'll end up with a link like this:
&lt;code&gt;https://raw.githubusercontent.com/massmutual/pvpc_map/master/data/pvpc_map/pvpc_towns.csv?token=AC33CYRi-2bMOocYNFelx_iQ9rccQDzcks5aC85gwA%3D%3D&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;which works just fine.&lt;/p&gt;
&lt;h2&gt;But, crucially, the token at the end of the URL above will go stale!&lt;/h2&gt;
&lt;p&gt;To get around this, we instead need to load the data from the GitHub Pages hosting here: &lt;code&gt;https://massmutual.github.io/pvpc_map/data/pvpc_map/pvpc_towns.csv&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This gives us a stable URL for the data file, and we're off to developing more visualization features.
Here is the CodePen for the map:
&lt;a href="https://codepen.io/MIDS-W209/pen/xLBrRy"&gt;https://codepen.io/MIDS-W209/pen/xLBrRy&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Welcome Olson Stanley Reagan</title><link href="/2017/10/11/welcome-olson-stanley-reagan/" rel="alternate"></link><published>2017-10-11T00:00:00+02:00</published><updated>2017-10-11T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-10-11:/2017/10/11/welcome-olson-stanley-reagan/</id><summary type="html">&lt;p&gt;Today my wife Sam gave birth to our first son, Olson Stanley.
He was born at 11:45AM, weighing 6 pounds and 9 ounces, at 20 some inches of length.&lt;/p&gt;
&lt;p&gt;Mom and baby are happy and healthy and we are excited for this new chapter in our lives!&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today my wife Sam gave birth to our first son, Olson Stanley.
He was born at 11:45AM, weighing 6 pounds and 9 ounces, at 20 some inches of length.&lt;/p&gt;
&lt;p&gt;Mom and baby are happy and healthy and we are excited for this new chapter in our lives!&lt;/p&gt;</content></entry><entry><title>A grad’s view: Solving real problems</title><link href="/2017/09/29/a-grads-view-solving-real-problems/" rel="alternate"></link><published>2017-09-29T00:00:00+02:00</published><updated>2017-09-29T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-09-29:/2017/09/29/a-grads-view-solving-real-problems/</id><summary type="html">&lt;p&gt;This is a re-port of an article I wrote for the MassMutal blog: &lt;a href="https://blog.massmutual.com/post/a-grads-view-solving-real-problems"&gt;https://blog.massmutual.com/post/a-grads-view-solving-real-problems&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Just this past May, I graduated from the University of Vermont and the Computational Story Lab research group to work as a senior data scientist with MassMutual. While there are many …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a re-port of an article I wrote for the MassMutal blog: &lt;a href="https://blog.massmutual.com/post/a-grads-view-solving-real-problems"&gt;https://blog.massmutual.com/post/a-grads-view-solving-real-problems&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Just this past May, I graduated from the University of Vermont and the Computational Story Lab research group to work as a senior data scientist with MassMutual. While there are many differences between being a lifelong student (read: PhD student) versus working in industry, the overlap is huge and I have been able to settle right in at MassMutual.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Differences&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The biggest difference is what defines value. Research in machine learning is valued by novel algorithms and marginal improvements in classification accuracies. But, in business, our data science efforts and machine learning algorithms provide value by making improvements to the customer experience.&lt;/p&gt;
&lt;p&gt;While spending months of effort developing an improved algorithm and moving accuracy from 90 percent to 92 percent would be heralded in academic journals, moving a process from 50 percent to 90 percent is where all the gains are. (&lt;em&gt;Related&lt;/em&gt;: &lt;a href="https://blog.massmutual.com/post/data-science-and-living-mutual"&gt;Data science and MassMutual&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Overlap&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;One of the biggest reasons that things are so similar is that the Data Science team at MassMutual has co-opted and applied many of the practices that work well at universities. I found it easy to feel at home with weekly lab meetings, a culture of learning from peers complete with a mentorship structure, and an interval review process for project write-ups akin to journal papers (without any, ahem, rude reviewers).&lt;/p&gt;
&lt;p&gt;Add this to the fact that the Computational Story Lab’s use of machine learning and computation are used as tools to study interesting problems, and it’s a match.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What universities could learn&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;On the flip side, what I have learned in my few months at MassMutual could go a long way to improving research.&lt;/p&gt;
&lt;p&gt;We have a Data Engineering team to support the computing and data platform (excitement from Day 1), track our work in a way that helps keep projects moving, follow a consistent coding style across the team, and ensure that our models are reproducible. The latter part, building reproducible models with a consistent project structure, has been an effort that I and another team member, Paul Shearer, worked on and implemented recently.&lt;/p&gt;
&lt;p&gt;Finally, while our focus is on solving business problems, that isn’t to say that there isn’t any science going on. Our models to predict longevity utilize new methods that I still haven’t wrapped my head around. We tweak state-of-the-practice methodology for tricky problems, and use experimentation to track the results of our models and improve them.&lt;/p&gt;
&lt;p&gt;We’re bringing applied machine learning to areas across the company, and it’s an exciting place to be!&lt;/p&gt;
&lt;p&gt;More from MassMutual…&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.massmutual.com/post/grad-school-and-problem-solving-at-massmutual"&gt;MassMutual’s grad program: How it works&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://datascience.massmutual.com/development"&gt;MassMutual’s Data Science Development Program&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://blog.massmutual.com/category/live-mutual"&gt;Live Mutual: Lessons and stories&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>Boston-bound for 2018</title><link href="/2017/09/10/boston-bound-for-2018/" rel="alternate"></link><published>2017-09-10T00:00:00+02:00</published><updated>2017-09-10T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-09-10:/2017/09/10/boston-bound-for-2018/</id><summary type="html">&lt;p&gt;Well, I never thought it would happen.
Today I qualified for the Boston Marathon with a 3:00:04 showing at the Presque Isle Marathon.&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/1180631963/embed/ec6353ad2f3fa0fcf9762dc78efa8bf53822bcb5'&gt;&lt;/iframe&gt;</summary><content type="html">&lt;p&gt;Well, I never thought it would happen.
Today I qualified for the Boston Marathon with a 3:00:04 showing at the Presque Isle Marathon.&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='https://www.strava.com/activities/1180631963/embed/ec6353ad2f3fa0fcf9762dc78efa8bf53822bcb5'&gt;&lt;/iframe&gt;</content></entry><entry><title>Call me Doctor :)</title><link href="/2017/05/20/call-me-doctor/" rel="alternate"></link><published>2017-05-20T00:00:00+02:00</published><updated>2017-05-20T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-05-20:/2017/05/20/call-me-doctor/</id><summary type="html">&lt;p&gt;Today I have completed my PhD at the University of Vermont, earning a Doctorate in Applied Mathematics along with a Certificate of Study in Complex Systems.
I can't offer enough thanks to my advisors, Peter Dodds and Chris Danforth, as well as my family and my wife, Sam.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today I have completed my PhD at the University of Vermont, earning a Doctorate in Applied Mathematics along with a Certificate of Study in Complex Systems.
I can't offer enough thanks to my advisors, Peter Dodds and Chris Danforth, as well as my family and my wife, Sam.&lt;/p&gt;</content></entry><entry><title>Enabling Jupyter notebook dashboards</title><link href="/2017/05/04/enabling-jupyter-notebook-dashboards/" rel="alternate"></link><published>2017-05-04T00:00:00+02:00</published><updated>2017-05-04T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-05-04:/2017/05/04/enabling-jupyter-notebook-dashboards/</id><summary type="html">&lt;p&gt;If you perform EDA using jupyter notebooks, it’s really easy to share those results with some moderate interaction via a jupyter dashboard. Here are the basic steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Build the analysis, etc. Assuming this is done locally. Install the dashboard layout extension and lay out some sweet graphs. Optional: decorate …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;If you perform EDA using jupyter notebooks, it’s really easy to share those results with some moderate interaction via a jupyter dashboard. Here are the basic steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Build the analysis, etc. Assuming this is done locally. Install the dashboard layout extension and lay out some sweet graphs. Optional: decorate some graph function with &lt;code&gt;@interact&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Commit to github/bitbucket. An example: git commit -am "so much wow"; git push github exploratory.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ssh to server. Pull commit down on the server.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Back to local. Scp the data files up to the server, in the same place they were locally. It comes in handy to have put them in a folder called &lt;code&gt;data&lt;/code&gt;. This looks like &lt;code&gt;scp -r data myserver:project-dir/&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ssh back to server. Install virtualenv, all necessary libs (numpy, pandas, etc).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the dashboard layout (you definitely need it now, if you didn’t do the layout in step 1): &lt;a href="https://github.com/jupyter/dashboards"&gt;https://github.com/jupyter/dashboards&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Enable widgets &lt;code&gt;jupyter nbextension enable --py --sys-prefix widgetsnbextension&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Launch server, point browser, choose dashboard view, run notebook!&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I’ve done this all in a screen environment to leave it running. Overhead should be minimal on the server. Before launching the notebook server, I put my necessary credentials into environment variables as well.&lt;/p&gt;
&lt;p&gt;Another potentially important step is to password protect the server. This can be done by calling &lt;code&gt;jupyter notebook password&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This way, you don’t need the other two dashboard pieces (the bundler or standalone server). Although if someone were to set those up, it could make this process even easier!&lt;/p&gt;</content></entry><entry><title>Should I set metadata manually in pyspark?</title><link href="/2017/05/04/should-i-set-metadata-manually-in-pyspark/" rel="alternate"></link><published>2017-05-04T00:00:00+02:00</published><updated>2017-05-04T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-05-04:/2017/05/04/should-i-set-metadata-manually-in-pyspark/</id><summary type="html">&lt;p&gt;Well, let’s do a simple test and find out if it speeds up the process of one-hot encoding a variable in our data. There are other reasons to set it, and we’ll get to those. Starting with the very helpful code snippet from &lt;a href="https://github.com/awesome-spark/spark-gotchas/blob/master/06_data_preparation.md"&gt;spark-gotchas&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;Well, let’s do a simple test and find out if it speeds up the process of one-hot encoding a variable in our data. There are other reasons to set it, and we’ll get to those. Starting with the very helpful code snippet from &lt;a href="https://github.com/awesome-spark/spark-gotchas/blob/master/06_data_preparation.md"&gt;spark-gotchas&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;json&lt;/span&gt;

&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Column&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;pyspark.sql.functions&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;withMeta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;meta&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;sc&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SparkContext&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_active_spark_context&lt;/span&gt;
    &lt;span class="n"&gt;jmeta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sc&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_gateway&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;jvm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;org&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;apache&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sql&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;types&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Metadata&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;getattr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;_jc&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;as&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)(&lt;/span&gt;&lt;span class="n"&gt;alias&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;jmeta&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fromJson&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dumps&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;meta&lt;/span&gt;&lt;span class="p"&gt;))))&lt;/span&gt;

&lt;span class="n"&gt;Column&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withMeta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;withMeta&lt;/span&gt;


&lt;span class="n"&gt;meta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ml_attr&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;label_with_meta&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;nominal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;vals&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;0.0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1.0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;2.0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]}}&lt;/span&gt;


&lt;span class="n"&gt;df_with_meta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withColumn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;label_with_meta&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;col&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;label&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;withMeta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;meta&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;df_with_meta&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;schema&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;metadata&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;meta&lt;/span&gt;

&lt;span class="c1"&gt;## True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;First, I will write functions to do a one hot encoding with and without metadata. Without metadata:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def testWithoutMetadata(df):
    OHEncoder = OneHotEncoder(inputCol=&amp;quot;label&amp;quot;,
                              outputCol=&amp;quot;label_OH&amp;quot;)

    df_transformed = OHEncoder.transform(df)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And with metadata:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;def testMetadata(df):
    meta = {&amp;quot;ml_attr&amp;quot;: {&amp;quot;name&amp;quot;: &amp;quot;label_with_meta&amp;quot;,
      &amp;quot;type&amp;quot;: &amp;quot;nominal&amp;quot;,
      &amp;quot;vals&amp;quot;: [&amp;quot;0&amp;quot;,&amp;quot;1&amp;quot;,&amp;quot;2&amp;quot;]}}
    OHEncoderMeta = OneHotEncoder(inputCol=&amp;quot;label_with_meta&amp;quot;,
                                  outputCol=&amp;quot;label_OH_with_meta&amp;quot;)

    df_with_meta = df.withColumn(&amp;quot;label_with_meta&amp;quot;,                    
                                 col(&amp;quot;label&amp;quot;).withMeta(&amp;quot;&amp;quot;, meta))
    df_with_meta_transformed = OHEncoderMeta.transform(df_with_meta)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You can see how in the above, I used the withMeta function from above.
Now, the tests:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;%%timeit -n 5 -r 5&lt;/span&gt;
&lt;span class="nb"&gt;size&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)))),[&lt;/span&gt;&amp;quot;&lt;span class="n"&gt;id&lt;/span&gt;&amp;quot;&lt;span class="p"&gt;,&lt;/span&gt; &amp;quot;&lt;span class="n"&gt;label&lt;/span&gt;&amp;quot;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;testWithoutMetadata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;%%timeit -n 5 -r 5&lt;/span&gt;
&lt;span class="nb"&gt;size&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;100000&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="n"&gt;spark&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;createDataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arange&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;&lt;span class="n"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="nb"&gt;size&lt;/span&gt;&lt;span class="p"&gt;)))),[&lt;/span&gt;&amp;quot;&lt;span class="n"&gt;id&lt;/span&gt;&amp;quot;&lt;span class="p"&gt;,&lt;/span&gt; &amp;quot;&lt;span class="n"&gt;label&lt;/span&gt;&amp;quot;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;testMetadata&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The speed difference for 100,000 data points: 4.66s to 4.16s. A half second speedup on a single executor. In essence, this amounts to how long it too spark to look up the range on the column for which we didn’t provide metadata (check the scala). Was it worth it? Probably not.&lt;/p&gt;
&lt;p&gt;Onward!&lt;/p&gt;
&lt;p&gt;Final note — there are still likely to be cases where the metadata is useful to spark, such as:
1. Avoiding using a stringIndexer, which is quite a bit slower than the one-hot encoder.
2. Providing the variable type to a classification method such as a Random Forest, which can take advantage of variable types.
3. Doing a one-hot encoding where the full range of values is not included in the training set. You can pass the known full range in as metadata, and your pipeline will reflect this.&lt;/p&gt;</content></entry><entry><title>Spring 2017 Berekely DataViz Project Roundup</title><link href="/2017/04/28/spring-2017-berekely-dataviz-project-roundup/" rel="alternate"></link><published>2017-04-28T00:00:00+02:00</published><updated>2017-04-28T00:00:00+02:00</updated><author><name>Andy Reagan</name></author><id>tag:None,2017-04-28:/2017/04/28/spring-2017-berekely-dataviz-project-roundup/</id><summary type="html">&lt;p&gt;Some really fantastic work from the teams in W209 this semester. Links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;https://rileyrustad.github.io/W209_Final/?demographic=Custom&lt;/li&gt;
&lt;li&gt;https://sharmila-velamur.github.io/trumpworld/home.html&lt;/li&gt;
&lt;li&gt;https://ramseynoj.github.io/history_of_refugee_migration/metrocosm/imigration-paths.html&lt;/li&gt;
&lt;li&gt;http://people.ischool.berkeley.edu/~yhzhao/food_access.html#dashboard_2&lt;/li&gt;
&lt;li&gt;http://people.ischool.berkeley.edu/~rthamman/W209_Project_Final …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Some really fantastic work from the teams in W209 this semester. Links:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;https://rileyrustad.github.io/W209_Final/?demographic=Custom&lt;/li&gt;
&lt;li&gt;https://sharmila-velamur.github.io/trumpworld/home.html&lt;/li&gt;
&lt;li&gt;https://ramseynoj.github.io/history_of_refugee_migration/metrocosm/imigration-paths.html&lt;/li&gt;
&lt;li&gt;http://people.ischool.berkeley.edu/~yhzhao/food_access.html#dashboard_2&lt;/li&gt;
&lt;li&gt;http://people.ischool.berkeley.edu/~rthamman/W209_Project_Final/Final.html&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>Modular papers with LaTeX</title><link href="/2017/03/23/modular-papers-with-latex/" rel="alternate"></link><published>2017-03-23T00:00:00+01:00</published><updated>2017-03-23T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-03-23:/2017/03/23/modular-papers-with-latex/</id><summary type="html">&lt;p&gt;Earlier this week I wrote about how to &lt;a href="https://medium.com/@andyreagan/structuring-academic-project-directories-3d959684fba1#.xvmvhqz1y"&gt;structure an academic project directory&lt;/a&gt; and one of those directories was &lt;code&gt;paper&lt;/code&gt;. The project write-ups (read: papers) will live here. My tool of choice for writing is LaTeX. It is not for everyone, and since you’re this far I’m going …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Earlier this week I wrote about how to &lt;a href="https://medium.com/@andyreagan/structuring-academic-project-directories-3d959684fba1#.xvmvhqz1y"&gt;structure an academic project directory&lt;/a&gt; and one of those directories was &lt;code&gt;paper&lt;/code&gt;. The project write-ups (read: papers) will live here. My tool of choice for writing is LaTeX. It is not for everyone, and since you’re this far I’m going to assume that you’re already a fan!&lt;/p&gt;
&lt;p&gt;While templates for many academic journals are available from &lt;a href="https://www.sharelatex.com/templates/journals/national-academy-of-sciences-%28pnas%29"&gt;ShareLaTeX&lt;/a&gt;, &lt;a href="https://www.overleaf.com/gallery/tagged/academic-journal"&gt;Overleaf&lt;/a&gt;, and often the journals themselves, they share a common problem: each template is a new file with the content copied into it. One of main reasons that we use LaTeX is the clean &lt;em&gt;separation of content and style&lt;/em&gt;, and having a new, single tex file for each journal violates this principle. We can overcome this by using multiple files for the content of our writing. To start, we can take a paper that has files looking like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emotional-arcs-EPJ-template.tex
bmc-mathphys.bst
bmcart.cls
everything.bib
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Into one that looks like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emotional-arcs-EPJ.tex
emotional-arcs.body.tex
emotional-arcs.abs.tex
bmc-mathphys.bst
bmcart.cls
everything.bib
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We’ve taken the both and the abstract out, and included them inside the EPJ template using &lt;code&gt;\input{\filenamebase.abs}&lt;/code&gt; and &lt;code&gt;\input{\filenamebase.body}&lt;/code&gt; where for convenience we define the working name of the paper only once in the file with &lt;code&gt;\newcommand{filenamebase}{emotional-arcs}&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Immediately, we can now easily switch between formats and rely on the same body and abstract files:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emotional-arcs-EPJ.tex
emotional-arcs-revtex4.tex
emotional-arcs.body.tex
emotional-arcs.abs.tex
bmc-mathphys.bst
bmcart.cls
unsrtabbrv.bst
everything.bib
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Expanding on this idea, we’ll end up with a directory that looks just like the first academic paper directory I ever encountered, one laid out by &lt;a href="http://www.uvm.edu/pdodds/"&gt;Peter Dodds&lt;/a&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bmc-mathphys.bst
bmcart.cls
emotional-arcs-EPJ-supp.tex
emotional-arcs-EPJ.tex
emotional-arcs-dissertation-supplementary.tex
emotional-arcs-dissertation.tex
emotional-arcs-revtex4.tex
emotional-arcs.abs.tex
emotional-arcs.acknowledgments.tex
emotional-arcs.author.EPJ.tex
emotional-arcs.author.tex
emotional-arcs.biblio.tex
emotional-arcs.body.captions.tex
emotional-arcs.body.figures.tex
emotional-arcs.body.nocite.tex
emotional-arcs.body.nofigures.tex
emotional-arcs.body.tables.tex
emotional-arcs.body.tex
emotional-arcs.coverletter.tex
emotional-arcs.inputs.txt
emotional-arcs.kwd.tex
emotional-arcs.nbooks.tex
emotional-arcs.settings.tex
emotional-arcs.supplementary.SOM.tex
emotional-arcs.supplementary.SVD-all.tex
emotional-arcs.supplementary.SVD.tex
emotional-arcs.supplementary.construction.tex
emotional-arcs.supplementary.extras.tex
emotional-arcs.supplementary.networks.tex
emotional-arcs.supplementary.null.tex
emotional-arcs.supplementary.prediction.tex
emotional-arcs.supplementary.ringsort.tex
emotional-arcs.supplementary.stories.tex
emotional-arcs.supplementary.tex
emotional-arcs.supplementary.wards-all.tex
emotional-arcs.supplementary.wards.tex
emotional-arcs.title.tex
everything.bib
makefile
unsrtabbrv.bst
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Wow, that’s a lot of files! And for your sake, I didn’t even include all of the automatically generated files. For starters, we can note that all five of these files are generated by a Python script from our body file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;emotional-arcs.body.captions.tex
emotional-arcs.body.figures.tex
emotional-arcs.body.nocite.tex
emotional-arcs.body.nofigures.tex
emotional-arcs.body.tables.tex
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That allowed us to include the figures after the main text in our EPJ template. For this procedure, on would run&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;python &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PYTOOLDIR&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;split_body.py &lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;PAPER&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;.body.tex
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;which a line straight from the makefile. The Python code to do this is over at: &lt;a href="https://github.com/andyreagan/kitchentabletools-python"&gt;https://github.com/andyreagan/kitchentabletools-python&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other things that are now possible include making a &lt;code&gt;-combined.tex&lt;/code&gt; file for pulling in all of the inputs (see &lt;code&gt;input-inputs.py&lt;/code&gt; for that script), and copying the combined file into a directory with the figures locally (renamed fig01,fig01, etc).&lt;/p&gt;
&lt;p&gt;Some of the &lt;code&gt;.tex&lt;/code&gt; files aren’t even ones that we wrote: Python also spit out some of the tables (&lt;code&gt;*.SVD-all.tex&lt;/code&gt;) and parameters (&lt;code&gt;*.nbooks.tex&lt;/code&gt;) from the analysis itself. Made an update to the code? No problem, the figures and parameter files are refreshed all we need to do is run &lt;code&gt;make all&lt;/code&gt; to build the paper!&lt;/p&gt;
&lt;p&gt;For a repository with an example layout, see &lt;a href="https://github.com/petersheridandodds/universal-paper-template"&gt;https://github.com/petersheridandodds/universal-paper-template&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Structuring academic project directories</title><link href="/2017/03/21/structuring-academic-project-directories/" rel="alternate"></link><published>2017-03-21T00:00:00+01:00</published><updated>2017-03-21T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-03-21:/2017/03/21/structuring-academic-project-directories/</id><summary type="html">&lt;p&gt;Organizing the basic structure of your computer is tremendously helpful in keeping track of things. You’re a good person, so I’ll start by assuming that you’re already using unix. Still, it’s easy to get overwhelmed when poking through old directories that full of folders named “attempt1 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Organizing the basic structure of your computer is tremendously helpful in keeping track of things. You’re a good person, so I’ll start by assuming that you’re already using unix. Still, it’s easy to get overwhelmed when poking through old directories that full of folders named “attempt1”, “attempt2”, etc. Ryan Gallagher, a fellow student from the &lt;a href="http://compstorylab.org/"&gt;Computational Story Lab&lt;/a&gt;, who is defending his Master’s Thesis today (good luck Ryan!), sums up the problem like this:&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;past me from June 21st is why we can&amp;#39;t have nice things &lt;a href="https://t.co/r5cMselk6G"&gt;pic.twitter.com/r5cMselk6G&lt;/a&gt;&lt;/p&gt;&amp;mdash; Ryan Gallagher (@ryanjgallag) &lt;a href="https://twitter.com/ryanjgallag/status/843845937097334784?ref_src=twsrc%5Etfw"&gt;March 20, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;In the great tradition of stealing, most of my setup is inspired by (read: copied from) my PhD Advisor &lt;a href="http://uvm.edu/pdodds/"&gt;Peter Dodds&lt;/a&gt;. Next, we’ll take a look at the project directories themselves.
For projects that I have worked on, the strategies employed are varied (of course), but the best structure I have found has a division of labour like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;.
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The root directory for the project. Inside here are all of the relevant folders. Don’t set up a git repo up here, that’s one thing that have the code/paper/etc in separate folders is going to help with.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bin/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For all of the code. If it’s Python, R, MATLAB, Perl, Julia, a Jupyter notebook…it belongs here. Now that the code is in it’s own folder, you can keep it under a clean version control, share with colleagues, and publish it to github for #openscience!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;paper/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;All of the tex files!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;data/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The data. The code can look for &lt;code&gt;../data&lt;/code&gt; to get everything it needs. Some caveats here: it’s often large (not to be included in GitHub), and not always in this directory. Code and data aren’t easy (or always sensible) to separate: YMMV.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;figures/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Written to by the scripts, and read from for the paper. Also, tables go here! You might as well call it “shared”.
And that’s it! Other folders that show up a lot at the top level include &lt;code&gt;presentations&lt;/code&gt;, &lt;code&gt;media&lt;/code&gt;, and &lt;code&gt;output&lt;/code&gt; for non-figure code output.&lt;/p&gt;</content></entry><entry><title>Home sweet Home</title><link href="/2017/03/16/home-sweet-home/" rel="alternate"></link><published>2017-03-16T00:00:00+01:00</published><updated>2017-03-16T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-03-16:/2017/03/16/home-sweet-home/</id><summary type="html">&lt;p&gt;Today, my wife and I took another step towards adulthood and closed on our first home.
Or, as I like to remind myself, the bank just bought another house and is letting us have the privilege of living in it and paying them interest to do so.
It was a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today, my wife and I took another step towards adulthood and closed on our first home.
Or, as I like to remind myself, the bank just bought another house and is letting us have the privilege of living in it and paying them interest to do so.
It was a bit of a wild ride, with Sam's train down to the closing from Burlington being delayed by the snow storm, and our closing being pushed back a day.
But signing the papers was simple (compared to what we'd heard!), many thanks to our attorney Donna Reidy, our realtor Chuck Berube, and our bank representative Gary Talbot.
If you are looking for recommendations, these people were great and I'd be happy to put you in touch.&lt;/p&gt;
&lt;p&gt;Before I get started ranting about amortization schedules and property taxes, I have some decks that need to be stained!&lt;/p&gt;</content></entry><entry><title>Digital Humanities in 2017</title><link href="/2017/01/11/digital-humanities-in-2017/" rel="alternate"></link><published>2017-01-11T00:00:00+01:00</published><updated>2017-01-11T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2017-01-11:/2017/01/11/digital-humanities-in-2017/</id><summary type="html">&lt;p&gt;I received this message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tyler
Jan 9&lt;/p&gt;
&lt;p&gt;Hello Mr. Reagan,&lt;/p&gt;
&lt;p&gt;Recently I read your paper "The emotional arcs of stories are dominated by six basic shapes", and it fascinated me. I'd love to learn more about how data scientists are able to deconstruct and graph language; where can I find …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;I received this message:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Tyler
Jan 9&lt;/p&gt;
&lt;p&gt;Hello Mr. Reagan,&lt;/p&gt;
&lt;p&gt;Recently I read your paper "The emotional arcs of stories are dominated by six basic shapes", and it fascinated me. I'd love to learn more about how data scientists are able to deconstruct and graph language; where can I find similar research papers like this one, in the fields of literature and computational/data science?&lt;/p&gt;
&lt;p&gt;Any help would be appreciated. Thanks in advance,
Tyler&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A quick response from me on some work to look into:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Andy Reagan
11:29 AM&lt;/p&gt;
&lt;p&gt;Tyler,&lt;/p&gt;
&lt;p&gt;Thanks for reaching out! There are a number of people working the area whose work is really interesting, and this area is broadly called the "digital humanities". That name, and some of the original ideas were inspired by Moretti (see the book "Distant Reading"). You could also start by checking out the work being done by Ben Schmidt, David Bamman, and David Elson- I've seen excellent papers from all of them.&lt;/p&gt;
&lt;p&gt;If you let me know more specifically what you're interested in, I might be able to provide some more specific resources.&lt;/p&gt;
&lt;p&gt;Cheers,
Andy&lt;/p&gt;
&lt;/blockquote&gt;</content></entry><entry><title>The Shapes of Stories</title><link href="/2016/11/07/the-shapes-of-stories/" rel="alternate"></link><published>2016-11-07T00:00:00+01:00</published><updated>2016-11-07T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-11-07:/2016/11/07/the-shapes-of-stories/</id><summary type="html">&lt;p&gt;This is a repost of my original at our &lt;a href="http://www.uvm.edu/storylab/2016/11/07/emotional-arcs/"&gt;StoryLab blog&lt;/a&gt;_&lt;/p&gt;
&lt;p&gt;Stories help us encode and understand our collective existence, underpin cultures, and help frame the possible. Describing the ecology of all human stories is an essential scientific enterprise. With the advent of the internet and massive digitization this …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is a repost of my original at our &lt;a href="http://www.uvm.edu/storylab/2016/11/07/emotional-arcs/"&gt;StoryLab blog&lt;/a&gt;_&lt;/p&gt;
&lt;p&gt;Stories help us encode and understand our collective existence, underpin cultures, and help frame the possible. Describing the ecology of all human stories is an essential scientific enterprise. With the advent of the internet and massive digitization this vital work has become, in part, a data-driven one.&lt;/p&gt;
&lt;p&gt;There are many aspects of stories to characterize and here we take on just one: The overall emotional trajectory.&lt;/p&gt;
&lt;p&gt;In a lecture recorded in 1985, Kurt Vonnegut introduced the idea of quantifying the emotional arcs of stories. He suggested that “Man-in-a-hole” is a primary kind of shape in the dimension of good-ill fortune. “Somebody gets into trouble… gets out of it again. People LOVE that story!” If you haven’t seen the video, it’s less than 5 minutes long and a rewarding experience:&lt;/p&gt;
&lt;iframe class="youtube-player" width="554" height="300" type="text/html" src="http://www.youtube.com/embed/oP3c1h8v2ZQ?version=3&amp;amp;rel=1&amp;amp;fs=1&amp;amp;autohide=2&amp;amp;showsearch=0&amp;amp;showinfo=1&amp;amp;iv_load_policy=1&amp;amp;wmode=transparent" allowfullscreen="true" style="border:0;"&gt;&lt;/iframe&gt;

&lt;p&gt;Vonnegut pointed out that computers would be perfectly suited to the task of finding good-ill fortune trajectories, and with this inspiration and today’s computing power, we tested his instincts on a large supply of books. We extracted and analyzed the emotional arcs of 1,722 novels from the Project Gutenberg corpus using sentiment analysis, and found six common shapes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Rags-to-riches,&lt;/li&gt;
&lt;li&gt;Tragedy,&lt;/li&gt;
&lt;li&gt;Man-in-a-hole (see below for more),&lt;/li&gt;
&lt;li&gt;Icarus,&lt;/li&gt;
&lt;li&gt;Cindarella,&lt;/li&gt;
&lt;li&gt;Oedipus.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here’s a graphical overview of our process:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.uvm.edu/storylab/wp-content/uploads/paper-schematic-150.png"&gt;&lt;img class="img-responsive" src="http://www.uvm.edu/storylab/wp-content/uploads/paper-schematic-150-760x1024.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It seems that in the space of all possible emotional arcs, we tend to prefer a handful of comprehensible building block shapes.&lt;/p&gt;
&lt;p&gt;We have interactive visualizations of the emotional arcs of thousands of books (and movies) at &lt;a href="http://hedonometer.org/books/v3/1777/"&gt;hedonometer.org&lt;/a&gt;. Details can be found in the publication in &lt;a href="http://link.springer.com/article/10.1140%2Fepjds%2Fs13688-016-0093-1"&gt;EPJ Data Science&lt;/a&gt; and the paper's &lt;a href="http://compstorylab.org/share/papers/reagan2016b/"&gt;online appendices&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;One more thing: Tomorrow is the 2016 US Presidential Election. Slogans are, when done well, powerful indicators of a larger narrative. The slogan "Make America Great Again", first used in &lt;a href="https://en.wikipedia.org/wiki/Make_America_Great_Again"&gt;Ronald Reagan's 1980 presidential campaign&lt;/a&gt; (and at times by other politicians including Bill Clinton), induces a remarkably full dynamic that matches Vonnegut's Man-in-a-hole trajectory. In fact, it's a much stronger framing as Man-in-a-hole does not suggest a trajectory through time by itself (&lt;a href="https://en.wikipedia.org/wiki/The_Metamorphosis"&gt;The Metamorphosis&lt;/a&gt; would roughly fit for example though the “hole” changes in nature). In four words, “Make America Great Again” manages to indicate a perceived state of the past and present, and a desired future:
&lt;a href="http://www.uvm.edu/storylab/wp-content/uploads/make-_-great-again-cropped-002.jpeg"&gt;&lt;img class="img-responsive" src="http://www.uvm.edu/storylab/wp-content/uploads/make-_-great-again-cropped-002-1024x489.jpeg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our stance here is apolitical—we are attempting to analyze stories scientifically, and we saw that “Make America Great Again” fits one of Vonnegut’s basic shapes.&lt;/p&gt;
&lt;p&gt;And our work on emotional arcs is just one part of understanding the ecology of stories.
There is much more to do: Extracting and comparing plots, character paths, comparing across cultures and time periods.
But all this now seems possible.&lt;/p&gt;</content></entry><entry><title>I'm Married!</title><link href="/2016/10/22/im-married/" rel="alternate"></link><published>2016-10-22T00:00:00+02:00</published><updated>2016-10-22T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-10-22:/2016/10/22/im-married/</id><summary type="html">&lt;p&gt;I'm happy to report that as of October 22nd, 2016, I'm happily married to the lovely Samantha Spisiak (@sspis1).
It was raining and in the 30's, in other words a perfect Vermont day, and here is one of my favorite pictures from the day:&lt;/p&gt;
&lt;div class="col-md-8 col-md-offset-2"&gt;
&lt;img src="/images/2016-10-22-hitched/hitched.png" class="img-responsive"&gt;
&lt;/div&gt;</summary><content type="html">&lt;p&gt;I'm happy to report that as of October 22nd, 2016, I'm happily married to the lovely Samantha Spisiak (@sspis1).
It was raining and in the 30's, in other words a perfect Vermont day, and here is one of my favorite pictures from the day:&lt;/p&gt;
&lt;div class="col-md-8 col-md-offset-2"&gt;
&lt;img src="/images/2016-10-22-hitched/hitched.png" class="img-responsive"&gt;
&lt;/div&gt;</content></entry><entry><title>O2X Summit Challenge!</title><link href="/2016/10/01/o2x-summit-challenge/" rel="alternate"></link><published>2016-10-01T00:00:00+02:00</published><updated>2016-10-01T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-10-01:/2016/10/01/o2x-summit-challenge/</id><summary type="html">&lt;p&gt;After grabbing a couple hours of sleep, I had an awesome time running the &lt;a href="https://o2x.com/summit-challenge/"&gt;O2X Summit Challenge&lt;/a&gt; on Saturday with &lt;a href="https://www.strava.com/athletes/642263"&gt;Chris Danforth&lt;/a&gt; and &lt;a href="https://www.strava.com/athletes/3824994"&gt;Jarlath O'Neil-Dunne&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here's my &lt;a href="https://www.strava.com/activities/730863922"&gt;Strava data&lt;/a&gt;.
The course was considerably more challenging (and interesting) than I'd expected- way more fun than running up the access road. There …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After grabbing a couple hours of sleep, I had an awesome time running the &lt;a href="https://o2x.com/summit-challenge/"&gt;O2X Summit Challenge&lt;/a&gt; on Saturday with &lt;a href="https://www.strava.com/athletes/642263"&gt;Chris Danforth&lt;/a&gt; and &lt;a href="https://www.strava.com/athletes/3824994"&gt;Jarlath O'Neil-Dunne&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here's my &lt;a href="https://www.strava.com/activities/730863922"&gt;Strava data&lt;/a&gt;.
The course was considerably more challenging (and interesting) than I'd expected- way more fun than running up the access road. There were ups, downs, lots of rough bushwhacking and even crawling through rocks, and up ladders!&lt;/p&gt;
&lt;p&gt;Perhaps the most fun part was the actual &lt;em&gt;racing&lt;/em&gt; that happened during the race: usually a 5k or 10k is a time trial (you go out, and run your own pace regardless of the other people), but near the top I was actually racing for the finish with two others! Putting in an attack that was quickly shut down, catching 3rd place again, and then a sprint finish. Awesome. I have only really &lt;em&gt;raced&lt;/em&gt; one other time in a triathlon/running race that I can remember (going to the AG in a Shelburne Sprint Tri years ago), and it's an entirely different experience (bike racing, for example, is always a head to head race...but running is different for us average people most of the time).&lt;/p&gt;
&lt;p&gt;I'll definitely be back!
Check out the Strava effort above for photos, here is my favorite:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2016-10-01-O2X/2016-O2X-team.jpg" class="img-responsive"&gt;&lt;/p&gt;</content></entry><entry><title>A record that needs breaking</title><link href="/2016/09/29/a-record-that-needs-breaking/" rel="alternate"></link><published>2016-09-29T00:00:00+02:00</published><updated>2016-09-29T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-09-29:/2016/09/29/a-record-that-needs-breaking/</id><summary type="html">&lt;p&gt;It's been about 5 years since I first stumbled across the Human Powered Land Speed [World] Record, and the competition for the record is certainly stiff.
It just got broken a &lt;a href="http://www.redbull.com/us/en/bike/stories/1331751185607/human-powered-speed-record-holder-interview"&gt;couple weeks ago&lt;/a&gt; with a run of 89MPH!!&lt;/p&gt;
&lt;p&gt;Looking through the records, I had originally noticed that the tandem …&lt;/p&gt;</summary><content type="html">&lt;p&gt;It's been about 5 years since I first stumbled across the Human Powered Land Speed [World] Record, and the competition for the record is certainly stiff.
It just got broken a &lt;a href="http://www.redbull.com/us/en/bike/stories/1331751185607/human-powered-speed-record-holder-interview"&gt;couple weeks ago&lt;/a&gt; with a run of 89MPH!!&lt;/p&gt;
&lt;p&gt;Looking through the records, I had originally noticed that the tandem record is &lt;em&gt;slower&lt;/em&gt; than the single person record...and since top speed is simply a matter of power overcoming drag, it should be faster!
Here's the &lt;a href="http://www.ihpva.org/hpvarecl.htm#nom12"&gt;record&lt;/a&gt; and some description of the &lt;a href="http://www.recumbents.com/wisil/whpsc2016/speedchallenge.htm"&gt;most recent event they had&lt;/a&gt;, for breaking records.&lt;/p&gt;
&lt;p&gt;I was able to find some details on the &lt;a href="http://www.recumbents.com/wisil/lem/glowworm/default.htm"&gt;current record holder&lt;/a&gt;, the &lt;a href="http://www.easyracers.com/racing.html"&gt;one before that&lt;/a&gt;, and &lt;a href="https://books.google.com/books?id=_VEEAAAAMBAJ&amp;amp;pg=PA85&amp;amp;lpg=PA85&amp;amp;dq=Vector+Tandem+al+voigt&amp;amp;source=bl&amp;amp;ots=I3brPbY16P&amp;amp;sig=8yxEvzYR3oYv8JAuGGcFdVnuXj0&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ved=0ahUKEwiplrmkp7XPAhUKLyYKHdZnDZoQ6AEIHjAA#v=onepage&amp;amp;q=Vector%20Tandem%20al%20voigt&amp;amp;f=false"&gt;from the 70's&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The back-to-back seems like an odd choice, but perhaps it makes the most since.
Either way, I'd love to take a shot at this one.&lt;/p&gt;</content></entry><entry><title>Javascript Vis Libraries</title><link href="/2016/09/26/javascript-vis-libraries/" rel="alternate"></link><published>2016-09-26T00:00:00+02:00</published><updated>2016-09-26T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-09-26:/2016/09/26/javascript-vis-libraries/</id><summary type="html">&lt;p&gt;Overlooking building a visualization using native DOM manipulation (or jQuery, React.js), there are a whole bunch of libraries to help make visualizations.
Here are just some:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://d3js.org/"&gt;d3.js&lt;/a&gt; BSD&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mbostock.github.io/protovis/"&gt;protoviz&lt;/a&gt; (d3.js predecessor)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dimplejs.org/"&gt;dimple.js&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://c3js.org/"&gt;C3.js&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://d3plus.org/"&gt;D3plus&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mozilla/metrics-graphics"&gt;Metrics Graphs …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Overlooking building a visualization using native DOM manipulation (or jQuery, React.js), there are a whole bunch of libraries to help make visualizations.
Here are just some:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://d3js.org/"&gt;d3.js&lt;/a&gt; BSD&lt;/li&gt;
&lt;li&gt;&lt;a href="http://mbostock.github.io/protovis/"&gt;protoviz&lt;/a&gt; (d3.js predecessor)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dimplejs.org/"&gt;dimple.js&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://c3js.org/"&gt;C3.js&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://d3plus.org/"&gt;D3plus&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/mozilla/metrics-graphics"&gt;Metrics Graphs&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://epochjs.github.io/epoch/"&gt;Epoch&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://vega.github.io/vega/"&gt;Vega&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://nvd3.org/examples/"&gt;NVD3&lt;/a&gt; (d3.js wrapper)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://processingjs.org/"&gt;processing.js&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://threejs.org/"&gt;three.js&lt;/a&gt; (* &lt;a href="http://data-arts.appspot.com/globe/"&gt;wow&lt;/a&gt;, and * &lt;a href="http://www.fallen.io/ww2/"&gt;fallen&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.pixijs.com/"&gt;PixiJS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://bokeh.pydata.org/en/latest/docs/user_guide/bokehjs.html#userguide-bokehjs"&gt;BokehJS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.highcharts.com/"&gt;Highcharts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.flotcharts.org/"&gt;Flot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.fusioncharts.com/"&gt;FusionCharts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://plot.ly/"&gt;Plotly&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.jfree.org/jfreechart/"&gt;JFreeChart&lt;/a&gt; LGPL&lt;/li&gt;
&lt;li&gt;&lt;a href=""&gt;Google Charts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://echarts.baidu.com/"&gt;Echarts (by Baidu)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sigmajs.org/"&gt;Sigma JS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/danvk/dygraphs"&gt;dygraphs&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.npmjs.com/search?q=data+visualization"&gt;and many more&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</content></entry><entry><title>AWS FAQ</title><link href="/2016/08/26/aws-faq/" rel="alternate"></link><published>2016-08-26T00:00:00+02:00</published><updated>2016-08-26T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-08-26:/2016/08/26/aws-faq/</id><summary type="html">&lt;p&gt;Always a little tricky to figure out the details of these things.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;list of instance details&lt;/a&gt;
&lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/block-device-mapping-concepts.html"&gt;block device mapping&lt;/a&gt;
^ scrolling way down, how to get metadata about the device
the bottom of this is super useful:
&lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html"&gt;aws guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;this &lt;a href="http://serverfault.com/questions/433703/how-to-use-instance-store-volumes-storage-in-amazon-ec2"&gt;helpful post&lt;/a&gt; clearly read the above.
I didn't find that I …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Always a little tricky to figure out the details of these things.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://aws.amazon.com/ec2/instance-types/"&gt;list of instance details&lt;/a&gt;
&lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/block-device-mapping-concepts.html"&gt;block device mapping&lt;/a&gt;
^ scrolling way down, how to get metadata about the device
the bottom of this is super useful:
&lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html"&gt;aws guide&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;this &lt;a href="http://serverfault.com/questions/433703/how-to-use-instance-store-volumes-storage-in-amazon-ec2"&gt;helpful post&lt;/a&gt; clearly read the above.
I didn't find that I needed to use mkfs for it work, the second drive just wasn't mounted.&lt;/p&gt;</content></entry><entry><title>Visualizing with scroll and D3</title><link href="/2016/08/08/visualizing-with-scroll-and-d3/" rel="alternate"></link><published>2016-08-08T00:00:00+02:00</published><updated>2016-08-08T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-08-08:/2016/08/08/visualizing-with-scroll-and-d3/</id><summary type="html">&lt;p&gt;Web technology has enabled the building of totally custom, interactive graphics with ease.
D3.js fills in some gaps by providing convienent scales and DOM manipulation that makes sense with data.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Web technology has enabled the building of totally custom, interactive graphics with ease.
D3.js fills in some gaps by providing convienent scales and DOM manipulation that makes sense with data.&lt;/p&gt;</content></entry><entry><title>D3.js and Reactjs</title><link href="/2016/08/05/d3js-and-reactjs/" rel="alternate"></link><published>2016-08-05T00:00:00+02:00</published><updated>2016-08-05T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-08-05:/2016/08/05/d3js-and-reactjs/</id><summary type="html">&lt;p&gt;The world of Javascript is complicated (some might say, insane).
I recently discovered that to use the newest version of Javascript (ES 2015), there are a whole suite of build tools and transpilers that will turn your ES2015 code into JS that can run on current (IE9+) browsers. Okay!&lt;/p&gt;
&lt;p&gt;A …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The world of Javascript is complicated (some might say, insane).
I recently discovered that to use the newest version of Javascript (ES 2015), there are a whole suite of build tools and transpilers that will turn your ES2015 code into JS that can run on current (IE9+) browsers. Okay!&lt;/p&gt;
&lt;p&gt;A popular new technology for from end web UI is Reactjs.
It's developed and released by Facebook, and solves problems for them by using components.
So, it must be useful for everyone, right?&lt;/p&gt;
&lt;p&gt;If you use D3 to build awesome web graphics, then you might think so.
But the problems that it solves are already solved by D3, namely DOM manipulation.
The following post goes into a few different ways of getting them to play together, but I'll throw up the caution flag that you're just making things more complicated.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://oli.me.uk/2015/09/09/d3-within-react-the-right-way/"&gt;D3 within React the right way&lt;/a&gt; by Oliver Caldwell.
Impressive work, and he totally understands how both of these technologies work, deeply. Kudos!&lt;/p&gt;
&lt;p&gt;Perhaps a benchmark of the performance gain or loss by these approaches would help answer if there is any reason at all, because it's not making the code much prettier.
I'll be sticking with the module pattern for building re-usable D3!&lt;/p&gt;
&lt;p&gt;Cheers!&lt;/p&gt;</content></entry><entry><title>Visiting the IBM Watson Research Center</title><link href="/2016/04/13/visiting-the-ibm-watson-research-center/" rel="alternate"></link><published>2016-04-13T00:00:00+02:00</published><updated>2016-04-13T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-04-13:/2016/04/13/visiting-the-ibm-watson-research-center/</id><summary type="html">&lt;p&gt;Fellow &lt;a href="http://compstorylab.org/"&gt;Computational Story Lab&lt;/a&gt; researcher &lt;a href="http://www.uvm.edu/~eclark/"&gt;Eric Clark&lt;/a&gt; is spending a few months working with scientists at the &lt;a href=""&gt;Watson Research Center&lt;/a&gt;.
The project that he is a part of is really exciting, and you can read more about ``Operation Blue Sky'' in the &lt;a href="http://www.forbes.com/sites/emilymullin/2016/04/07/pfizer-ibm-launch-ambitious-internet-of-things-for-parkinsons-research/#da9309c24d62"&gt;Forbes press release&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I went down for a …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Fellow &lt;a href="http://compstorylab.org/"&gt;Computational Story Lab&lt;/a&gt; researcher &lt;a href="http://www.uvm.edu/~eclark/"&gt;Eric Clark&lt;/a&gt; is spending a few months working with scientists at the &lt;a href=""&gt;Watson Research Center&lt;/a&gt;.
The project that he is a part of is really exciting, and you can read more about ``Operation Blue Sky'' in the &lt;a href="http://www.forbes.com/sites/emilymullin/2016/04/07/pfizer-ibm-launch-ambitious-internet-of-things-for-parkinsons-research/#da9309c24d62"&gt;Forbes press release&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I went down for a day to give a talk to their weekly machine learning seminar about some of the research that I've been doing with Chris Danforth and Peter Dodds at the `Lab.
In particular, I got to share some of the latest work we've been doing to visualize happiness and understand the stories in data.&lt;/p&gt;
&lt;p&gt;The research center and the people there are amazing, and the projects they have going are fascinating.
You aren't allowed to take any pictures though, so you'll have to take my word for it!
It was a pleasure to speak, tour the center, and meet the people on Eric's team.&lt;/p&gt;</content></entry><entry><title>Novel data assimilation improvements for limited observations</title><link href="/2016/02/11/novel-data-assimilation-improvements-for-limited-observations/" rel="alternate"></link><published>2016-02-11T00:00:00+01:00</published><updated>2016-02-11T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-02-11:/2016/02/11/novel-data-assimilation-improvements-for-limited-observations/</id><summary type="html">&lt;p&gt;NOTE: This is a repost of the original on the &lt;a href="http://www.uvm.edu/storylab/2016/02/11/novel-data-assimilation-improvements-for-limited-observations/"&gt;Computational Story Lab blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The availability of data on the current state of Earth's atmosphere/ocean/land system continues to improve. As the state-of-the-art weather models and supercomputing power allow for higher resolution forecasts, down to 1km resolution on massively …&lt;/p&gt;</summary><content type="html">&lt;p&gt;NOTE: This is a repost of the original on the &lt;a href="http://www.uvm.edu/storylab/2016/02/11/novel-data-assimilation-improvements-for-limited-observations/"&gt;Computational Story Lab blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The availability of data on the current state of Earth's atmosphere/ocean/land system continues to improve. As the state-of-the-art weather models and supercomputing power allow for higher resolution forecasts, down to 1km resolution on massively parallel computers, data assimilation techniques are needed to quickly combine the mass of available data.&lt;/p&gt;
&lt;p&gt;Here at the University of Vermont, we employ a &lt;a href="http://www.uvm.edu/storylab/2013/03/18/chaos-in-an-atmosphere-hanging-on-a-wall/" target="_blank"&gt;CFD model of a thermosyphon&lt;/a&gt; as a testbed for data assimilation techniques. Our latest research, published in &lt;a href="https://t.co/dzjUyyszhE" target="_blank"&gt;PLoS ONE&lt;/a&gt;, shows that by using information about the direction of uncertainty propagation within a forecast ensemble can improve prediction skill.&lt;/p&gt;
&lt;p&gt;In order to utilize an adaptively localized covariance, we combine a Ensemble Transform Kalman Filter (ETKF) with model estimates of flow velocity to adapt the localization parameters. Below we see an ensemble of initially random forecasts come closer to the true model state, using data assimilation.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.uvm.edu/storylab/wp-content/uploads/CaiJXdlW0AAIkfe.png" rel="attachment wp-att-1707"&gt;&lt;img class="aligncenter wp-image-1707" src="http://www.uvm.edu/storylab/wp-content/uploads/CaiJXdlW0AAIkfe.png" alt="CaiJXdlW0AAIkfe" width="600" height="441" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Modest improvements in forecast skill are shown to be possible, pushing the envelope of prediction just a bit further into the future. For more detail, take a look at the open-access paper: &lt;a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0148134"&gt;http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0148134&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>Using github pages for a project</title><link href="/2016/01/12/using-github-pages-for-a-project/" rel="alternate"></link><published>2016-01-12T00:00:00+01:00</published><updated>2016-01-12T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2016-01-12:/2016/01/12/using-github-pages-for-a-project/</id><summary type="html">&lt;p&gt;If you're unfamiliar with git, start by reading this: &lt;a href="http://tom.preston-werner.com/2009/05/19/the-git-parable.html"&gt;the git parable&lt;/a&gt;. It was recommended to my by Prof. Jim Bagrow, and I wasted a lot of time trying to understand git before reading it.&lt;/p&gt;
&lt;p&gt;Now, we're going to use git (and the git hosting site: github) to version control …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you're unfamiliar with git, start by reading this: &lt;a href="http://tom.preston-werner.com/2009/05/19/the-git-parable.html"&gt;the git parable&lt;/a&gt;. It was recommended to my by Prof. Jim Bagrow, and I wasted a lot of time trying to understand git before reading it.&lt;/p&gt;
&lt;p&gt;Now, we're going to use git (and the git hosting site: github) to version control and share our project. First things first, if you don't want to share your code to world right away, since you're a student go ahead and grab the &lt;a href="http://andyreagan.github.io/2016/01/12/using-github-pages-for-a-project/"&gt;github student pack&lt;/a&gt; (thanks Mark for pointing this out). Among other things, it includes 5 free private repositories hosted on github.&lt;/p&gt;
&lt;p&gt;Getting right to it, if you're not using git already in your project:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git init
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and then go ahead and just add everything (but don't include your huge (&amp;gt;50MB) data files).&lt;/p&gt;
&lt;p&gt;Do your first commit, connect to github, and push your master branch up to github. If that's unclear, there are better explanations on the web than any I could offer (see the parable above).&lt;/p&gt;
&lt;p&gt;Now, create a new branch like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git checkout --orphan gh-pages
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I found that here: &lt;a href="http://bitflop.com/tutorials/how-to-create-a-new-and-empty-branch-in-git.html"&gt;random git help&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;And carefully remove everything in it that you don't want on the website version. (Keep stuff you're sharing, and don't remove stuff not tracked by git!). I won't fully recommend this command: &lt;code&gt;\rm -rf *&lt;/code&gt;. Now, commit that branch, and push it to github:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git commit -am &amp;quot;first commit of gh-pages branch
git push origin gh-pages
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(Assuming you added github as origin).&lt;/p&gt;
&lt;p&gt;You now have a github pages project for your repo! Your URL (consult the &lt;a href="https://help.github.com/articles/user-organization-and-project-pages"&gt;github docs&lt;/a&gt; if you want to know more) is &lt;code&gt;http(s)://&amp;lt;orgname&amp;gt;.github.io/&amp;lt;projectname&amp;gt;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;You're done! Checkout master to keep working, grab files from that into gh-pages (checkout gh-pages and do &lt;code&gt;git checkout master myfile&lt;/code&gt;), create an index.html in the root of your gh-pages, have fun!&lt;/p&gt;</content></entry><entry><title>All about deployment</title><link href="/2015/08/08/all-about-deployment/" rel="alternate"></link><published>2015-08-08T00:00:00+02:00</published><updated>2015-08-08T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-08-08:/2015/08/08/all-about-deployment/</id><summary type="html">&lt;p&gt;The tools of the current day full-stack data science grad student come from the land of software developers, they have bizzare names, are changing constantly, and sometimes have super powers.
The grand pubah of reproducible research would be a mac mini shipped with every publication, and once turned on, would …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The tools of the current day full-stack data science grad student come from the land of software developers, they have bizzare names, are changing constantly, and sometimes have super powers.
The grand pubah of reproducible research would be a mac mini shipped with every publication, and once turned on, would make the whole paper, from scratch.
This is unreasonable, of course, so we settle for things like version control and virtual environments.&lt;/p&gt;
&lt;p&gt;To this end, the most complete level of isolation can be achieved with a living, breathing virtual machine.
With Vagrant, Ansible, and of course Git, we can do it.&lt;/p&gt;
&lt;p&gt;Vagrant: use a shell script to provision, or install in ssh session and rebox?
I'll try using a shell script, but this road seems dangerous.&lt;/p&gt;
&lt;p&gt;Fine tuning with documentation.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.ansible.com/home"&gt;Ansible&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://uwsgi-docs.readthedocs.org/en/latest/Python.html?highlight=virtualenv"&gt;UWSGI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://docs.vagrantup.com/v2/getting-started/index.html"&gt;Vagrant&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>Make every for-loop parallel in Julia</title><link href="/2015/07/20/make-every-for-loop-parallel-in-julia/" rel="alternate"></link><published>2015-07-20T00:00:00+02:00</published><updated>2015-07-20T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-07-20:/2015/07/20/make-every-for-loop-parallel-in-julia/</id><summary type="html">&lt;p&gt;If reading the &lt;a href="http://julia.readthedocs.org/"&gt;documentation&lt;/a&gt; is confusing, making any for-loop into a parallel loop is rediculously easy.
Just add the parallel macro!
If it's code that needs to execute before moving on, just pass an operator to to macro.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;# non-parallel loop&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;some_function …&lt;/span&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;If reading the &lt;a href="http://julia.readthedocs.org/"&gt;documentation&lt;/a&gt; is confusing, making any for-loop into a parallel loop is rediculously easy.
Just add the parallel macro!
If it's code that needs to execute before moving on, just pass an operator to to macro.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;# non-parallel loop&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;some_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and this becomes&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;# parallel loop&lt;/span&gt;
&lt;span class="nd"&gt;@parallel&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;some_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;, you're done!
If you find that the code moves on too quickly, just add an operator to force it to wait:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;# parallel loop&lt;/span&gt;
&lt;span class="n"&gt;my_zero_result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nd"&gt;@parallel&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;
    &lt;span class="n"&gt;println&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_array&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;some_function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="mi"&gt;0&lt;/span&gt;
&lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Julia!&lt;/p&gt;</content></entry><entry><title>Cross repository version control, using symbolic links</title><link href="/2015/07/09/cross-repository-version-control-using-symbolic-links/" rel="alternate"></link><published>2015-07-09T00:00:00+02:00</published><updated>2015-07-09T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-07-09:/2015/07/09/cross-repository-version-control-using-symbolic-links/</id><summary type="html">&lt;p&gt;So, you've been writing lots and lots of awesome code.
As your codebase amasses, you can rely on snippets that you've already written, in different projects, by copying them around and upgrading as you go: productivity++.
And, because you're a good person, you use &lt;code&gt;git&lt;/code&gt; to version control your projects …&lt;/p&gt;</summary><content type="html">&lt;p&gt;So, you've been writing lots and lots of awesome code.
As your codebase amasses, you can rely on snippets that you've already written, in different projects, by copying them around and upgrading as you go: productivity++.
And, because you're a good person, you use &lt;code&gt;git&lt;/code&gt; to version control your projects.&lt;/p&gt;
&lt;p&gt;But now, you have so many cool functions that you want to use them everything, and are left with 20 different versions across projects, each with the their own goodies.
Combining them into one library is painstaking, but you do it, and import that library into each of your projects.&lt;/p&gt;
&lt;p&gt;Every time you upgrade it for a project, you have to load the latest version into each project, and make sure it all still works.
The pain of updating and versioning your own code has overwhelmed you.&lt;/p&gt;
&lt;p&gt;So you put all the shared code into it's own project, and symbolically link it from all of your other projects.
Success!
But now, when you upload to github, and deploy through git, only the link get's included, and not the file.
Damnit!&lt;/p&gt;
&lt;p&gt;Git doesn't follow symbolic links and this is generally good practice.
But, you need it to.
Here is my super ugly solution: use the &lt;code&gt;.git/hooks/pre-commit&lt;/code&gt; and &lt;code&gt;post-commit&lt;/code&gt; to replace the links with the files, commit, then replaces the files back with the links!
Only problem now is that &lt;code&gt;git&lt;/code&gt; thinks that the mode has changed between every commit...too bad.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# .git/hooks/pre-commit
#!/bin/sh
LINKED_DIR=/Users/andyreagan/work/2015/07-hedotools/js
cd $GIT_DIR
cd ..
for FILE in d3.andy.js hedotools.init.js hedotools.shifter.js topojson.js urllib.js
do
\rm js/$FILE
cp $LINKED_DIR/$FILE js
git add js/$FILE
done
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# .git/hooks/post-commit
#!/bin/sh
LINKED_DIR=/Users/andyreagan/work/2015/07-hedotools/js
cd $GIT_DIR
cd ../js

# now, copy over the files
for FILE in d3.andy.js hedotools.init.js hedotools.shifter.js topojson.js urllib.js
do
\rm $FILE
ln -s $LINKED_DIR/$FILE $FILE
done
&lt;/pre&gt;&lt;/div&gt;</content></entry><entry><title>Light at the end of the tunnel</title><link href="/2015/05/03/light-at-the-end-of-the-tunnel/" rel="alternate"></link><published>2015-05-03T00:00:00+02:00</published><updated>2015-05-03T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-05-03:/2015/05/03/light-at-the-end-of-the-tunnel/</id><summary type="html">&lt;p&gt;With the semester nearing an end (one final exam taken, two presentations to give next week)...professor Bagrow sums up how I feel:&lt;/p&gt;
&lt;blockquote class="twitter-tweet" lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;Is there a thing like jet lag but for the semester? Feel like I could sleep for a month!&lt;/p&gt;&amp;mdash; Jim Bagrow (@bagrow) &lt;a href="https://twitter.com/bagrow/status/594217499652009985"&gt;May 1, 2015&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;A random …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With the semester nearing an end (one final exam taken, two presentations to give next week)...professor Bagrow sums up how I feel:&lt;/p&gt;
&lt;blockquote class="twitter-tweet" lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;Is there a thing like jet lag but for the semester? Feel like I could sleep for a month!&lt;/p&gt;&amp;mdash; Jim Bagrow (@bagrow) &lt;a href="https://twitter.com/bagrow/status/594217499652009985"&gt;May 1, 2015&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;A random subselection of highlights over the past two months:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sam's business, &lt;a href="http://wildflowerstudiobtv.com/"&gt;Wildflower Studios&lt;/a&gt; has secured and loan, a space, a web designer (guess who), and will be opening in September&lt;/li&gt;
&lt;li&gt;a self-declared &lt;a href="https://twitter.com/andyreagan/status/591370586498170881"&gt;win&lt;/a&gt; at the student research conference&lt;/li&gt;
&lt;li&gt;a few excellent bike rides including a &lt;a href="https://twitter.com/andyreagan/status/588035987957252096"&gt;crit race&lt;/a&gt;, and an attempt at the &lt;a href="https://www.strava.com/activities/296998303"&gt;6 gap&lt;/a&gt; ride&lt;/li&gt;
&lt;li&gt;and lots of &lt;a href="https://twitter.com/andyreagan/status/583058314201841664"&gt;science&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And looking forward to presentating at the 2015 SIAM Conference on Dynamical Systems, weddings, research, and bike riding this summer!&lt;/p&gt;</content></entry><entry><title>labMTsimple: upgrades!</title><link href="/2015/03/10/labmtsimple-upgrades/" rel="alternate"></link><published>2015-03-10T00:00:00+01:00</published><updated>2015-03-10T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-03-10:/2015/03/10/labmtsimple-upgrades/</id><summary type="html">&lt;p&gt;As part of an ongoing effort to evaluate different sentiment analysis tools, I've pushed a handful of upgrades to labMTsimple.
Namely:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Python 3 compatible&lt;/li&gt;
&lt;li&gt;New module "speedy," aiming for speed (also using numpy)&lt;/li&gt;
&lt;li&gt;Class-based sentiment object&lt;/li&gt;
&lt;li&gt;ANEW, MPQA, and Liu's dictionaries are included&lt;/li&gt;
&lt;li&gt;Using "marisa-trie" and "datrie" for fast prefix …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;As part of an ongoing effort to evaluate different sentiment analysis tools, I've pushed a handful of upgrades to labMTsimple.
Namely:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Python 3 compatible&lt;/li&gt;
&lt;li&gt;New module "speedy," aiming for speed (also using numpy)&lt;/li&gt;
&lt;li&gt;Class-based sentiment object&lt;/li&gt;
&lt;li&gt;ANEW, MPQA, and Liu's dictionaries are included&lt;/li&gt;
&lt;li&gt;Using "marisa-trie" and "datrie" for fast prefix matching&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It's all available on &lt;a href="https://github.com/andyreagan/labMT-simple"&gt;github&lt;/a&gt; as well as on &lt;a href="https://pypi.python.org/pypi/labMTsimple/2.2.2.1"&gt;pypi&lt;/a&gt; for installation via pip.&lt;/p&gt;
&lt;p&gt;Of course, the vectors produced for the labMT set in english by the speedy module are ordered the same.
Also, no other languages for labMT on speedy yet.&lt;/p&gt;
&lt;p&gt;Stay tuned for the results of performance tests from these other dictionaries on a variety of corpuses.&lt;/p&gt;</content></entry><entry><title>The big happy</title><link href="/2015/02/28/the-big-happy/" rel="alternate"></link><published>2015-02-28T00:00:00+01:00</published><updated>2015-02-28T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-02-28:/2015/02/28/the-big-happy/</id><summary type="html">&lt;p&gt;This month the latest, big happy, research paper from our group the Computational Storylab was published (&lt;a href="http://www.pnas.org/content/early/2015/02/04/1411678112"&gt;journal link&lt;/a&gt;).
In the press, this paper has been very strong, with lots of coverage that includes the New York Times, CBS, CNN, and Science magazine.
We've collated some of the most popular articles …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This month the latest, big happy, research paper from our group the Computational Storylab was published (&lt;a href="http://www.pnas.org/content/early/2015/02/04/1411678112"&gt;journal link&lt;/a&gt;).
In the press, this paper has been very strong, with lots of coverage that includes the New York Times, CBS, CNN, and Science magazine.
We've collated some of the most popular articles, and I've put them on the hedonometer &lt;a href="http://hedonometer.org/press.html"&gt;press page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;All this coverage has meant lots of extra work for some people involved with paper, myself included.
The interviews of my advisors Peter Dodds and Chris Danforth have shown up in each of the press pieces, which has left me to scramble together a couple visualizations.&lt;/p&gt;
&lt;p&gt;In particular, the week before the article appeared in the NYT, I've been working hard to update the city and state rankings with the final 2014 numbers, and doing a one-off anaylsis of the NYT by section.
With word vectors in hand from Jake Williams who parsed the NYT data with perl magic, I put together a showcase that ranks and compares the individual sections of the times.&lt;/p&gt;
&lt;p&gt;Here's an image that links to full visualization:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://hedonometer.org/showcase/nyt/" target="_blank"&gt;&lt;img src="/images/2015-02-28-new-hedonometer-viz/2015-02-28-nyt-viz-screen.png" class="img-responsive"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Most recently, we learned this past Monday that John Tierney of the NYT was going to be interviewed on CBS This Morning and I spent most of the day putting together a comparsion of the anchors.
They sent us the complete transcript of their show since the beginning of 2015, and I put all of the tools that I've built to use in making a speedy viz.
This is online here, with the video of Tierney being intervied at the bottom:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://hedonometer.org/showcase/cbs/" target="_blank"&gt;&lt;img src="/images/2015-02-28-new-hedonometer-viz/2015-02-28-cbs-viz-screen.png" class="img-responsive"&gt;&lt;/a&gt;&lt;/p&gt;</content></entry><entry><title>Visualizing Neural Networks</title><link href="/2015/02/14/visualizing-neural-networks/" rel="alternate"></link><published>2015-02-14T00:00:00+01:00</published><updated>2015-02-14T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-02-14:/2015/02/14/visualizing-neural-networks/</id><summary type="html">&lt;p&gt;This semester I'm taking Applied Artificial Neural Network with Professor Donna Rizzo, and I'm learning a lot about what the heck neural network are (and what they're good for).
In addition to coding up the algorithms in MATLAB, we've been encouraged to explore what's going on to build our intiutions …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This semester I'm taking Applied Artificial Neural Network with Professor Donna Rizzo, and I'm learning a lot about what the heck neural network are (and what they're good for).
In addition to coding up the algorithms in MATLAB, we've been encouraged to explore what's going on to build our intiutions of the different approaches.
So, after getting it to work in MATLAB, I coded a Bidirectional Associative Memory network in Javascript and used some d3 action to make a plot of the behavior.&lt;/p&gt;
&lt;p&gt;Here is a look at the state space of a few differnt, with lines showing the tranjectories taken under iteration of the network.
Starting at any given node with state A, the weight matrix W sends you to a new state B along a black line.
Then the transpose of the wieght matrix W sends your back from B along the green dashed line.
For different training data (different memories coded into the weight matrix) we get very different behavior.&lt;/p&gt;
&lt;p&gt;3D binary state space with stable associated memories:&lt;/p&gt;
&lt;p&gt;&lt;img src="/demos/2015-02AANN/network1.png" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;3D binary state space with unstable associated memories:&lt;/p&gt;
&lt;p&gt;&lt;img src="/demos/2015-02AANN/network2.png" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;5D binary state space with an unstable highly associated memory:&lt;/p&gt;
&lt;p&gt;&lt;img src="/demos/2015-02AANN/network3.png" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;You can check out a &lt;a href="http://andyreagan.github.io/demos/2015-02AANN/BAM.html"&gt;live example here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Of course, the full code is available under hw03 of the &lt;a href="https://github.com/andyreagan/CSYS359"&gt;github&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Towards the certificate of study in complex systems</title><link href="/2015/02/01/towards-the-certificate-of-study-in-complex-systems/" rel="alternate"></link><published>2015-02-01T00:00:00+01:00</published><updated>2015-02-01T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-02-01:/2015/02/01/towards-the-certificate-of-study-in-complex-systems/</id><summary type="html">&lt;p&gt;I'm taking three classes this semester to finish my coursework for the Certificate of Study in Complex Systems.
Having recently finished my quals, these classes will round out my plan of study with 1.5 years left to write and teach.
The classes that I'm taking are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evolutionary Robotics with …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;I'm taking three classes this semester to finish my coursework for the Certificate of Study in Complex Systems.
Having recently finished my quals, these classes will round out my plan of study with 1.5 years left to write and teach.
The classes that I'm taking are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Evolutionary Robotics with Professor Josh Bongard&lt;/li&gt;
&lt;li&gt;Modeling Complex Systems with Professor Maggie Eppstein&lt;/li&gt;
&lt;li&gt;Applied Artificial Neural Network with Professor Donna Rizzo&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I'm really excited about each of these courses!
I get to first build robots and think about cognition, then model things that are interesting in the real world, and finally learn about how computers learn.&lt;/p&gt;
&lt;p&gt;The class projects that I'm going to take on in each course can hopefully fit together and if they prove to be worthwhile, could turn into the papers that I'll need to write to finish my PhD.
I have a lot of ideas that will be kept top secret, but stay tuned!&lt;/p&gt;
&lt;p&gt;All of my code for the courses is going up on github (full transparency!) and you can check each of those out here, referenced by their UVM course number:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/andyreagan/ludobots"&gt;CSYS295&lt;/a&gt; also online on &lt;a href="http://www.reddit.com/r/ludobots/comments/2rs8tu/take_this_online_robotics_course_alongside/"&gt;reddit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/andyreagan/CSYS302"&gt;CSYS302&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/andyreagan/CSYS359"&gt;CSYS359&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content></entry><entry><title>Officially a Ph.D candidate!</title><link href="/2015/01/26/officially-a-phd-candidate/" rel="alternate"></link><published>2015-01-26T00:00:00+01:00</published><updated>2015-01-26T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-01-26:/2015/01/26/officially-a-phd-candidate/</id><summary type="html">&lt;p&gt;I've just recieved word of my passing the ODE/PDE qual, my third and final big qualifying exam.
The exam was very difficult but I had studied for three months and was able to stumble my way through.&lt;/p&gt;
&lt;p&gt;Now, the plan is to finish classes this semester (taking 3!) and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've just recieved word of my passing the ODE/PDE qual, my third and final big qualifying exam.
The exam was very difficult but I had studied for three months and was able to stumble my way through.&lt;/p&gt;
&lt;p&gt;Now, the plan is to finish classes this semester (taking 3!) and then write some papers as a first author!&lt;/p&gt;</content></entry><entry><title>Burlington Generator</title><link href="/2015/01/24/burlington-generator/" rel="alternate"></link><published>2015-01-24T00:00:00+01:00</published><updated>2015-01-24T00:00:00+01:00</updated><author><name>andy reagan</name></author><id>tag:None,2015-01-24:/2015/01/24/burlington-generator/</id><summary type="html">&lt;p&gt;After enacting &lt;a href="https://twitter.com/andyreagan/status/558726551379136512"&gt;part 1&lt;/a&gt; and &lt;a href="https://twitter.com/andyreagan/status/558775826703855618"&gt;part 2&lt;/a&gt; of my PhD qualifying exam recovery plan, I spent Saturday exploring Burlington's new makerspace.
There is a lot of info about the space on their &lt;a href="http://generatorvt.com/"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As UVM students, Curtis Saunders and I are enjoying a free membership through the exchange program.
I'm …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After enacting &lt;a href="https://twitter.com/andyreagan/status/558726551379136512"&gt;part 1&lt;/a&gt; and &lt;a href="https://twitter.com/andyreagan/status/558775826703855618"&gt;part 2&lt;/a&gt; of my PhD qualifying exam recovery plan, I spent Saturday exploring Burlington's new makerspace.
There is a lot of info about the space on their &lt;a href="http://generatorvt.com/"&gt;website&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As UVM students, Curtis Saunders and I are enjoying a free membership through the exchange program.
I'm really looking forward to using the space for some projects that I've been thinking about!
First up: a sign for Sam and a proper trophy for the Storylab Strava leaderboard.&lt;/p&gt;</content></entry><entry><title>The 5x(ODE exam)x4</title><link href="/2014/10/14/the-5xode-examx4/" rel="alternate"></link><published>2014-10-14T00:00:00+02:00</published><updated>2014-10-14T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2014-10-14:/2014/10/14/the-5xode-examx4/</id><summary type="html">&lt;p&gt;Leg 1:&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='http://www.strava.com/activities/207403252/embed/0fcc3758bfe2392e3d6ed082c0b16c6df9fd61a8'&gt;&lt;/iframe&gt;

&lt;p&gt;Leg 2:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2014-10-14-ode-run/odeexam.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Leg 3:&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='http://www.strava.com/activities/207403240/embed/a45e94e634501143eaab8154cc6feb6930c3e239'&gt;&lt;/iframe&gt;</summary><content type="html">&lt;p&gt;Leg 1:&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='http://www.strava.com/activities/207403252/embed/0fcc3758bfe2392e3d6ed082c0b16c6df9fd61a8'&gt;&lt;/iframe&gt;

&lt;p&gt;Leg 2:&lt;/p&gt;
&lt;p&gt;&lt;img src="/images/2014-10-14-ode-run/odeexam.jpg" class="img-responsive"&gt;&lt;/p&gt;
&lt;p&gt;Leg 3:&lt;/p&gt;
&lt;iframe height='405' width='590' frameborder='0' allowtransparency='true' scrolling='no' src='http://www.strava.com/activities/207403240/embed/a45e94e634501143eaab8154cc6feb6930c3e239'&gt;&lt;/iframe&gt;</content></entry><entry><title>Developing with hedonometer.org</title><link href="/2014/10/02/developing-with-hedonometerorg/" rel="alternate"></link><published>2014-10-02T00:00:00+02:00</published><updated>2014-10-02T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2014-10-02:/2014/10/02/developing-with-hedonometerorg/</id><summary type="html">&lt;p&gt;~This is a draft.~&lt;/p&gt;
&lt;p&gt;Edit: 2019-12-30 this &lt;em&gt;was&lt;/em&gt; a draft, I'm just going to publish it.&lt;/p&gt;
&lt;p&gt;Long story short, you'll need to clone the git repository on bitbucket (if you don't have a bitbucket account let me know and I'll move it to github, they're the same kind of thing …&lt;/p&gt;</summary><content type="html">&lt;p&gt;~This is a draft.~&lt;/p&gt;
&lt;p&gt;Edit: 2019-12-30 this &lt;em&gt;was&lt;/em&gt; a draft, I'm just going to publish it.&lt;/p&gt;
&lt;p&gt;Long story short, you'll need to clone the git repository on bitbucket (if you don't have a bitbucket account let me know and I'll move it to github, they're the same kind of thing).
Then you can add a remote that is the linode, (you ssh keys are already in all of the new user accounts on the linode under which the apps run), change the code, and then push to the linode.&lt;/p&gt;
&lt;p&gt;For me editing the master branch (this is fine since you're just changing text) looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git pull origin master &lt;span class="c1"&gt;# update the local version&lt;/span&gt;
&lt;span class="nb"&gt;source&lt;/span&gt; pyenv/bin/activate &lt;span class="c1"&gt;# initialize python virtual environment&lt;/span&gt;
./manage.py runserver &lt;span class="m"&gt;54043&lt;/span&gt;
&lt;span class="c1"&gt;# edit the code&lt;/span&gt;
&lt;span class="c1"&gt;# all the changes are viewable at 127.0.0.1:54043&lt;/span&gt;
C-c &lt;span class="c1"&gt;# to close the server&lt;/span&gt;
git status &lt;span class="c1"&gt;# check what you changed&lt;/span&gt;
git add newfile.html &lt;span class="c1"&gt;# add any new files&lt;/span&gt;
git commit -am &lt;span class="s2"&gt;&amp;quot;simple commit&amp;quot;&lt;/span&gt; &lt;span class="c1"&gt;# commit all changes to watched files&lt;/span&gt;
git push origin master &lt;span class="c1"&gt;# push to the bitbucket&lt;/span&gt;
git push production master &lt;span class="c1"&gt;# push to the linode&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The remote should look like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;happy% git remote -v
origin https://andrewreagan@bitbucket.org/andrewreagan/linode-backup.git &lt;span class="o"&gt;(&lt;/span&gt;fetch&lt;span class="o"&gt;)&lt;/span&gt;
origin https://andrewreagan@bitbucket.org/andrewreagan/linode-backup.git &lt;span class="o"&gt;(&lt;/span&gt;push&lt;span class="o"&gt;)&lt;/span&gt;
production prod@hedonometer.org:prod.git &lt;span class="o"&gt;(&lt;/span&gt;fetch&lt;span class="o"&gt;)&lt;/span&gt;
production prod@hedonometer.org:prod.git &lt;span class="o"&gt;(&lt;/span&gt;push&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;test&lt;/span&gt; dev@hedonometer.org:dev.git &lt;span class="o"&gt;(&lt;/span&gt;fetch&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;test&lt;/span&gt; dev@hedonometer.org:dev.git &lt;span class="o"&gt;(&lt;/span&gt;push&lt;span class="o"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Which is setup with &lt;code&gt;git remote add production prod@hedonometer.org:prod.git&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The specific files that you'll want to edit are in &lt;code&gt;hedonometer/templates/hedonometer/*.html&lt;/code&gt;.
Once you've cloned the repository, you can set up the virtual python environment using &lt;code&gt;virtualenv pyenv&lt;/code&gt;, installed from &lt;code&gt;brew install virtualenv&lt;/code&gt;. Then source the virtualenv, and use pip to install the following:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Django&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="mf"&gt;1.6&lt;/span&gt;
&lt;span class="n"&gt;South&lt;/span&gt;
&lt;span class="n"&gt;django&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;tastypie&lt;/span&gt;
&lt;span class="n"&gt;django&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sekizai&lt;/span&gt;
&lt;span class="n"&gt;MySQL&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;python&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Oh, also, for the site to run locally, you need a local mysql server and to put the connection settings into your environment. Getting the mysql server running (installed with &lt;code&gt;brew install mysql&lt;/code&gt;) was annoying at best, but then once you've got it running and created a database, my local environment settings are in a .env file in the repository:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_SECRET_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;******
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DEBUG&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;django.db.backends.mysql
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;testdb
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_USER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;root
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;arret3
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_HOST&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_PORT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_STATIC_ROOT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/Users/andyreagan/work/2014/2014-09hedonometer/mysite/static
&lt;span class="nb"&gt;alias&lt;/span&gt; &lt;span class="nv"&gt;minify&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;yuicompressor&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So...wow that sounds a lot more complicated than I thought it would be.&lt;/p&gt;
&lt;p&gt;A lot of the fuss is to get the site running locally, but as long as you don't need to see changes until they're on the site, you wouldn't need to get the site running locally...you could just clone the git repository, and push changes up the site.&lt;/p&gt;</content></entry><entry><title>Django on Linode: My Deployment Strategy</title><link href="/2014/09/30/django-on-linode-my-deployment-strategy/" rel="alternate"></link><published>2014-09-30T00:00:00+02:00</published><updated>2014-10-06T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2014-09-30:/2014/09/30/django-on-linode-my-deployment-strategy/</id><summary type="html">&lt;p&gt;Over the past six months, I definitely haven't figured out the best Django deployment strategy, but I've come a long way. The following is all set up on a &lt;a href="https://www.linode.com/"&gt;linode&lt;/a&gt; running Ubuntu 12.04.5 LTS.
There are a few key considerations to the setup of hedonometer.org:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We're using …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;p&gt;Over the past six months, I definitely haven't figured out the best Django deployment strategy, but I've come a long way. The following is all set up on a &lt;a href="https://www.linode.com/"&gt;linode&lt;/a&gt; running Ubuntu 12.04.5 LTS.
There are a few key considerations to the setup of hedonometer.org:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We're using a virtual machine for full control over the server&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt;, because it's a python MVC framework and has good templating&lt;/li&gt;
&lt;li&gt;Data files stored statically for visualizations (asynchronous loads)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://d3js.org/"&gt;d3&lt;/a&gt; for the data visualization&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Evolution of the server&lt;/h2&gt;
&lt;p&gt;At first, the whole thing was running off of the django development server, directly through nginx.
This lasted a long time.
To edit code on the server...it was edited live, through emacs (requiring root acess and port 22 open (at least only ssh keys)) and the django development server was restarted to show changes.
Static files we're collected, they were just edited in place, since this created another step in the way of editing.
Everything for the app was in &lt;code&gt;/usr/share/nginx/wiki/mysite&lt;/code&gt; due to some terrible folder creation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hedonometer.org -&amp;gt; nginx -&amp;gt; django
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Too many times the site went down due to typos while editing, so I moved to having two separate folders, one served by nginx through dev.hedonometer.org, and the production through hedonometer.org.
This worked well enough, but the code was still edited on the server, in place, and the security was still lacking.
The files were now separated out in &lt;code&gt;/usr/share/nginx/dev&lt;/code&gt;, &lt;code&gt;/usr/share/nginx/prod&lt;/code&gt;, &lt;code&gt;/usr/share/nginx/data&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hedonometer.org -&amp;gt; nginx -&amp;gt; django
dev.hedonometer.org -&amp;gt; nginx -&amp;gt; django
hedonometer.org/data -&amp;gt; nginx -&amp;gt; /usr/share/nginx/data
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then along came uWSGI. This got us away from the django development server, a good step forward...but we were just running two uwsgi servers from root.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;hedonometer.org -&amp;gt; nginx -&amp;gt; uwsgi -&amp;gt; django
dev.hedonometer.org -&amp;gt; nginx -&amp;gt; uwsgi -&amp;gt; django
hedonometer.org/data -&amp;gt; nginx -&amp;gt; /usr/share/nginx/data
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;With a great leap, I spawned uWSGI in emporer mode, created user accounts with empty git repositories for each version of the site, and could push the site up through git, where uwsgi would restart with post commit hook, static files would be collected, etc.
The user accounts were &lt;code&gt;prod&lt;/code&gt; and &lt;code&gt;dev&lt;/code&gt;, so the site was hosted in &lt;code&gt;/home/prod/hedonometer&lt;/code&gt; and &lt;code&gt;/home/dev/hedonometer&lt;/code&gt;.
Getting a local version running with mysql on the mac was a pain, but then I could actually run the site locally.
And things were better.
Since most of the data is loaded from static data files, I started using full URL paths for the data.&lt;/p&gt;
&lt;p&gt;And that leaves us at now.
A final upgrade removed all of the private settings in python files, and moved them all to the environment...so that the whole project could be shared on &lt;a href="https://github.com/andyreagan/hedonometer"&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Setting up this strategy&lt;/h2&gt;
&lt;p&gt;First, rip through the &lt;a href="https://www.linode.com/docs/security/securing-your-server"&gt;security guide&lt;/a&gt; from linode. By that, I mean take a good couple days.
Then:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.linode.com/docs/websites/nginx/websites-with-nginx-on-ubuntu-12-04-lts-precise-pangolin"&gt;Set up nginx&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.linode.com/docs/databases/mysql/using-mysql-relational-databases-on-ubuntu-12-04-lts-precise-pangolin"&gt;Get mysql running&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Basically, we're going to create a user account, some settings for uwsgi and nginx to serve this account, and start the bare git repository that will be used to host it.
I've pulled these settings together from a lot of different places, mainly the docs for each service, and I also want to acknowledge that I found a &lt;a href="http://bradenmacdonald.com/blog/2013/hosting-django-apps-ubuntu-nginx-uwsgi"&gt;blog post by Braden MacDonald&lt;/a&gt; that has a very similar strategy.
All of this can be accomplised with a long bash script, which I'll post someday, and here is the blow by blow:&lt;/p&gt;
&lt;p&gt;Since you followed the linode security guide, log in to your user account on the linode. For me, this is &lt;code&gt;user0&lt;/code&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh user0@hedonometer.org
su root
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We're going to create an app called &lt;code&gt;storybreaker&lt;/code&gt;, and serve it at &lt;code&gt;storybreaker.hedonometer.org&lt;/code&gt;. First, make it a database:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;echo &amp;quot;create database storybreaker&amp;quot; | mysql --user=root --password=&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;DJ_DB_PASSWORD&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Create a user account:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;useradd -d /home/storybreaker -G www-data -m -U -s /bin/bash storybreaker
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Log into that user account and make the git repo.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;su storybreaker &lt;span class="c1"&gt;# log in as storybreaker&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt;
mkdir .ssh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; chmod &lt;span class="m"&gt;700&lt;/span&gt; .ssh &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; touch .ssh/authorized_keys
mkdir &lt;span class="nv"&gt;$USER&lt;/span&gt;.git
&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="nv"&gt;$USER&lt;/span&gt;.git
git init --bare
&lt;span class="nb"&gt;cd&lt;/span&gt; ..
mkdir storybreaker
mkdir uwsgi
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, while you're in as &lt;code&gt;storybreaker&lt;/code&gt;, edit the post recieve hook in &lt;code&gt;~/storybreaker.git/hooks/post-recieve&lt;/code&gt; to do some stuff:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;GIT_WORK_TREE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/home/storybreaker/storybreaker
git checkout -f

python /home/storybreaker/storybreaker/manage.py collectstatic --noinput

&lt;span class="nb"&gt;cd&lt;/span&gt; ~/uwsgi
cp config&lt;span class="o"&gt;{&lt;/span&gt;.base,.tmp&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;env = DJ_SECRET_KEY=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DJ_SECRET_KEY&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; config.tmp
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;env = DJ_DEBUG=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DJ_DEBUG&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; config.tmp
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;env = DJ_DB_ENGINE=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DJ_DB_ENGINE&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; config.tmp
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;env = DJ_DB_NAME=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DJ_DB_NAME&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; config.tmp
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;env = DJ_DB_USER=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DJ_DB_USER&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; config.tmp
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;env = DJ_DB_PASSWORD=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DJ_DB_PASSWORD&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; config.tmp
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;env = DJ_DB_HOST=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DJ_DB_HOST&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; config.tmp
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;env = DJ_DB_PORT=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DJ_DB_PORT&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; config.tmp
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;env = DJ_STATIC_ROOT=&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;DJ_STATIC_ROOT&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; config.tmp
cp config&lt;span class="o"&gt;{&lt;/span&gt;.tmp,&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;where the &lt;code&gt;~/uwsgi/config.base&lt;/code&gt; looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;[&lt;/span&gt;uwsgi&lt;span class="o"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# setting from braden&lt;/span&gt;
&lt;span class="nv"&gt;socket&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /home/storybreaker/uwsgi/socket
chmod-socket &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;666&lt;/span&gt;
&lt;span class="nv"&gt;master&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;true&lt;/span&gt;
&lt;span class="nv"&gt;processes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="m"&gt;10&lt;/span&gt;

&lt;span class="c1"&gt;# for python&lt;/span&gt;
&lt;span class="nv"&gt;virtualenv&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /home/storybreaker/storybreaker/pyenv
&lt;span class="nv"&gt;pythonpath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /home/storybreaker/storybreaker
&lt;span class="nv"&gt;module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; mysite.wsgi

&lt;span class="nv"&gt;pidfile2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /home/storybreaker/uwsgi/pid
&lt;span class="nv"&gt;daemonize&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; /home/storybreaker/uwsgi/log
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Make sure to get a newline at the end of the &lt;code&gt;config.base&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;And the &lt;code&gt;.env&lt;/code&gt; file storying the settings for the app looks like (make sure this is sourced in &lt;code&gt;~/.bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# /home/storybreaker/.env&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_SECRET_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;not telling&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DEBUG&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;FALSE
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_ENGINE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;django.db.backends.mysql
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;storybreaker
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_USER&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;root
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;not telling either&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_HOST&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;127&lt;/span&gt;.0.0.1
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_DB_PORT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;3306&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;DJ_STATIC_ROOT&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;/home/storybreaker/storybreaker/mysite/static
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So now you can see what the post recieve hook does: copy over the files, collect static, and make a new config file for uWSGI.
Once this new config file is copied over, the server will restart, because we're about to link to it the folder that the uWSGI emporer is watching.&lt;/p&gt;
&lt;p&gt;Copy over the ssh keys to their account:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cat ~/.ssh/authorized_keys &amp;gt;&amp;gt; /home/storybreaker/.ssh/authorized_keys
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now is a good time to push up the app, test the database connection, and install the requirements in the &lt;code&gt;virtualenv&lt;/code&gt;.
To push from the local repo:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# locally&lt;/span&gt;
git remote add linode storybreaker@hedonometer.org:storybreaker.git
git push linode master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I store the requirements in a file called &lt;code&gt;requirements.txt&lt;/code&gt;, and so setting up from here (under the storybreaker user) looks like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# as storybreaker&lt;/span&gt;
env &lt;span class="c1"&gt;# check the the DJ_ settings are in the environment&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; ~/storybreaker
./manage.py dbshell &lt;span class="c1"&gt;# make sure we can log into the db&lt;/span&gt;
./manage.py collectstatic &lt;span class="c1"&gt;# check static file collection&lt;/span&gt;
virtualenv pyenv &lt;span class="c1"&gt;# set up virtualenv&lt;/span&gt;
. pyenv/bin/activate
pip install -r requirements.txt
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, if everything worked, we just need to create and link a nginx configuration, and link the uwsgi configuration.
The nginx config file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# the upstream component nginx needs to connect to&lt;/span&gt;
upstream storybreaker &lt;span class="o"&gt;{&lt;/span&gt;
    server unix:///home/storybreaker/uwsgi/socket&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;# for a file socket&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# configuration of the server&lt;/span&gt;
server &lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;# the port your site will be served on&lt;/span&gt;
    listen      &lt;span class="m"&gt;80&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="c1"&gt;# the domain name it will serve for&lt;/span&gt;
    server_name storybreaker.hedonometer.org&lt;span class="p"&gt;;&lt;/span&gt;
    charset     utf-8&lt;span class="p"&gt;;&lt;/span&gt;

    &lt;span class="c1"&gt;# max upload size&lt;/span&gt;
    client_max_body_size 75M&lt;span class="p"&gt;;&lt;/span&gt;   &lt;span class="c1"&gt;# adjust to taste&lt;/span&gt;
    &lt;span class="c1"&gt;# set this for local development&lt;/span&gt;
    &lt;span class="c1"&gt;# add_header &amp;#39;Access-Control-Allow-Origin&amp;#39; &amp;#39;http://127.0.0.1:54043&amp;#39;;&lt;/span&gt;

    rewrite  ^/&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="se"&gt;\?&lt;/span&gt;.*&lt;span class="o"&gt;)&lt;/span&gt;?$  /index.html&lt;span class="nv"&gt;$1&lt;/span&gt;  permanent&lt;span class="p"&gt;;&lt;/span&gt;

    location /static &lt;span class="o"&gt;{&lt;/span&gt;
        autoindex on&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="nb"&gt;alias&lt;/span&gt; /home/storybreaker/storybreaker/mysite/static&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;# your Django project&amp;#39;s static files - amend as required&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    location /data &lt;span class="o"&gt;{&lt;/span&gt;
        autoindex on&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="nb"&gt;alias&lt;/span&gt; /usr/share/nginx/data&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;# your Django project&amp;#39;s static files - amend as required&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;

    &lt;span class="c1"&gt;# Finally, send all non-media requests to the Django server.&lt;/span&gt;
    location / &lt;span class="o"&gt;{&lt;/span&gt;
        uwsgi_pass storybreaker&lt;span class="p"&gt;;&lt;/span&gt;
        include /home/storybreaker/uwsgi_params&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;# the uwsgi_params file you installed&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# link this config&lt;/span&gt;
ln -s /home/storybreaker/nginx.conf /etc/nginx/sites-enabled/storybreaker
&lt;/pre&gt;&lt;/div&gt;


&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# from root, copy over this file we made&lt;/span&gt;
mv /home/panometer/uwsgi/config /etc/uwsgi/panometer.ini
&lt;span class="c1"&gt;# double check that they own this&lt;/span&gt;
chown storbreaker:www-data /etc/uwsgi/panometer.ini
&lt;span class="c1"&gt;# as panometer, make a link&lt;/span&gt;
su panometer
ln -s /etc/uwsgi/panometer.ini ~/uwsgi/config
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, just restart nginx and it should be working!&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;nginx -s reload
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Things I'm looking to do&lt;/h2&gt;
&lt;p&gt;To avoid duplicating, there are a whole host of issues that I still have, and I've posted them on the github repository: &lt;a href="https://github.com/andyreagan/hedonometer/issues"&gt;https://github.com/andyreagan/hedonometer/issues&lt;/a&gt;.&lt;/p&gt;</content></entry><entry><title>Hedonometer 2.0</title><link href="/2014/09/29/hedonometer-20/" rel="alternate"></link><published>2014-09-29T00:00:00+02:00</published><updated>2014-09-29T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2014-09-29:/2014/09/29/hedonometer-20/</id><summary type="html">&lt;p&gt;This is reblog of &lt;a href="http://www.uvm.edu/storylab/"&gt;Computational Story Lab&lt;/a&gt;'s post title &lt;a href="http://www.uvm.edu/storylab/2014/09/29/hedonometer-2-0/"&gt;Hedonometer 2.0&lt;/a&gt; written by &lt;a href="http://www.uvm.edu/~pdodds/"&gt;Peter Dodds&lt;/a&gt;.&lt;/p&gt;
&lt;div id="attachment_1431" class="wp-caption alignleft"&gt;&lt;a href="http://hedonometer.org/maps.html"&gt;&lt;img src="http://compstorylab.org/wp-content/uploads/2014-09-15hedonometer-map-1024x764.png" alt="Geography of Happiness for the US" class="size-large wp-image-1431 img-responsive" /&gt;&lt;/a&gt;&lt;p class="wp-caption-text"&gt;Geography of Happiness for the US&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Over the summer of 2014, we have worked very hard to bring many new pieces to our &lt;a href="http://www.hedonometer.org/"&gt;Hedonometer&lt;/a&gt;, and we’re pleased to tell you about what …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is reblog of &lt;a href="http://www.uvm.edu/storylab/"&gt;Computational Story Lab&lt;/a&gt;'s post title &lt;a href="http://www.uvm.edu/storylab/2014/09/29/hedonometer-2-0/"&gt;Hedonometer 2.0&lt;/a&gt; written by &lt;a href="http://www.uvm.edu/~pdodds/"&gt;Peter Dodds&lt;/a&gt;.&lt;/p&gt;
&lt;div id="attachment_1431" class="wp-caption alignleft"&gt;&lt;a href="http://hedonometer.org/maps.html"&gt;&lt;img src="http://compstorylab.org/wp-content/uploads/2014-09-15hedonometer-map-1024x764.png" alt="Geography of Happiness for the US" class="size-large wp-image-1431 img-responsive" /&gt;&lt;/a&gt;&lt;p class="wp-caption-text"&gt;Geography of Happiness for the US&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;Over the summer of 2014, we have worked very hard to bring many new pieces to our &lt;a href="http://www.hedonometer.org/"&gt;Hedonometer&lt;/a&gt;, and we’re pleased to tell you about what we’ve done, and where we’re going next.&lt;/p&gt;
&lt;div id="attachment_1433" class="wp-caption alignleft"&gt;&lt;a href="http://hedonometer.org"&gt;&lt;img src="http://compstorylab.org/wp-content/uploads/2014-09-11hedonometer-time-series-300x175.png" alt="Snapshot of Hedonometer 2.0&amp;#039;s happiness time series." class="size-medium wp-image-1433 img-responsive" /&gt;&lt;/a&gt;&lt;p class="wp-caption-text"&gt;Snapshot of Hedonometer 2.0&amp;#8242;s happiness time series.&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;All along, one of the central goals for the Hedonometer has been to provide a new instrument for society’s dashboard, one that measures population-level happiness in real time from any streaming text source. Like flying a plane, where we would never want just one dial with the limits “all good” and “uh-oh”, we need a sophisticated dashboard to quantify how well a population is faring. We want to see unconventional measures like ours added to traditional, easier-to-gauge quantities often concerned with economic activity. Money &lt;a href="http://en.wikipedia.org/wiki/Hedonic_treadmill"&gt;doesn’t&lt;/a&gt; &lt;a href="http://en.wikipedia.org/wiki/Easterlin_paradox"&gt;equal&lt;/a&gt; happiness. We hope the Hedonometer will enable individuals, journalists, policy makers, corporations, and other research teams in their various pursuits.&lt;/p&gt;
&lt;p&gt;Because our Hedonometer works for any large text, we’re able to explore other areas for basic science purposes, particularly the vast realm of sociotechnical systems and the &lt;a href="http://www.hedonometer.org/books.html"&gt;digital humanities&lt;/a&gt;. And some of our work will be simply just for fun (hopefully yours and ours).&lt;/p&gt;
&lt;div id="attachment_1436" class="wp-caption alignright"&gt;&lt;a href="http://hedonometer.org/books.html?book=Harry%20Potter%20and%20the%20Prisoner%20of%20Azkaban"&gt;&lt;img src="http://compstorylab.org/wp-content/uploads/2014-09-27prisoner-of-azkaban-300x203.png" alt="Harry Potter and the Prisoner of Azkaban." class="size-medium wp-image-1436 img-responsive" /&gt;&lt;/a&gt;&lt;p class="wp-caption-text"&gt;Harry Potter and the Prisoner of Azkaban.&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;As you’ll see below, we have many plans for the future. So far, we’ve received crucial support from the NSF and the MITRE Corporation, and we’re always looking for more ways to continue to lift our enterprise. If you’re interested in or have suggestions about funding our work, &lt;a href="http://hedonometer.org/funding.html"&gt;please contact us&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Okay—here’s what we’ve put together. We now have four main interactive views of emotion up and running:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A completely rebuilt global Twitter &lt;a href="http://www.hedonometer.org/index.html"&gt;happiness time series in English&lt;/a&gt;, updated daily and with powerful new word shifts;&lt;/li&gt;
&lt;li&gt;An interactive &lt;a href="http://www.hedonometer.org/maps.html"&gt;map of happiness&lt;/a&gt; for the 50 US States plus DC, also based on Twitter;&lt;/li&gt;
&lt;li&gt;A &lt;a href="http://www.hedonometer.org/cities.html"&gt;ranked list of cities&lt;/a&gt; by happiness for the US (Twitter again);&lt;/li&gt;
&lt;li&gt;and an explorable visualization of &lt;a href="http://www.hedonometer.org/books.html"&gt;the emotional plot trajectories of 10,000 books in 10 major languages&lt;/a&gt; including &lt;a href="http://hedonometer.org/books.html?book=Harry%20Potter%20and%20the%20Sorcerer%27s%20Stone"&gt;Harry Potter&lt;/a&gt; along with classic and obscure works.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We’ll go into more depth about how to use and share these visualizations in our following blog posts. As always, Hedonometer stands on a &lt;a href="http://hedonometer.org/about.html"&gt;team effort&lt;/a&gt; but we have to acknowledge and praise Andy Reagan (&lt;a href="http://www.uvm.edu/storylab/2014/09/29/hedonometer-2-0/www.twitter.com/@andyreagan"&gt;@andyreagan&lt;/a&gt;) for his incredible efforts in leading the charge to Hedonometer 2.0. Building things is fun.&lt;/p&gt;
&lt;p&gt;Some of the many new elements we’re looking to add in the next year are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Other kinds of real-time, population-scale meters based on word usage including sleep, food consumption, exercise, binge-drinking, and boredom. We’ll apply these meters to Twitter but they could in principle be used on any text.&lt;/li&gt;
&lt;li&gt;Global Twitter happiness time series and maps for all 10 languages: English, Spanish, French, German, Brazilian Portuguese, Indonesian, Korean, Simplified Chinese, Arabic, and Russian;&lt;/li&gt;
&lt;li&gt;Real-time Twitter happiness at 1 minute time scales.&lt;/li&gt;
&lt;li&gt;An interactive world map with the ability to explore at scales of country, state, city, and district (or equivalents).&lt;/li&gt;
&lt;li&gt;Simple ways to embed our interactive visualizations into webpages;&lt;/li&gt;
&lt;li&gt;A simple interface for uploading and comparing two texts, and for generating shareable visualizations;&lt;/li&gt;
&lt;li&gt;Phrase-based rather than word-based analysis in English;&lt;/li&gt;
&lt;li&gt;More stand-alone projects such as interactive visualizations of music lyrics over the last 60 years;&lt;/li&gt;
&lt;li&gt;Measures based on other major emotions such as fear, disgust, anger, and surprise.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We also have two longer-term, major projects in development:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A fast search facility in the Hedonometer for users to find the emotional spectrum around specific words or phrases. This is a computationally bundensome problem. We’ll be able to show how the emotional texture of how people are talking about an event or a product.&lt;/li&gt;
&lt;li&gt;Storybreaker: a real-time extractor of stories and narratives emerging around major events. Our algorithm will include emotion but our goal is to measure frames around issues and ultimately meaningful stories.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One last thing: we’ve moved our blog from &lt;a href="http://onehappybird.com/"&gt;onehappybird&lt;/a&gt; to &lt;a href="http://www.compstorylab.org/"&gt;compstorylab.org&lt;/a&gt;. All old links will still work.&lt;/p&gt;</content></entry><entry><title>Using labMT-simple to make wordshifts</title><link href="/2014/09/24/using-labmt-simple-to-make-wordshifts/" rel="alternate"></link><published>2014-09-24T00:00:00+02:00</published><updated>2014-09-24T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2014-09-24:/2014/09/24/using-labmt-simple-to-make-wordshifts/</id><summary type="html">&lt;p&gt;I just merged updates to the d3 wordshift plotting into labMTsimple, and combined with phantom crowbar (see previous post), it's easier than ever to use the labMT data set to compare texts.&lt;/p&gt;
&lt;p&gt;To make an html page with the shift, you'll just need to have labMT-simple installed.
To automate the …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just merged updates to the d3 wordshift plotting into labMTsimple, and combined with phantom crowbar (see previous post), it's easier than ever to use the labMT data set to compare texts.&lt;/p&gt;
&lt;p&gt;To make an html page with the shift, you'll just need to have labMT-simple installed.
To automate the process into generating svg files, you'll need the phantom crowbar, which depends on phantomjs.
To go all the way to pdf, you'll also need inkscape.&lt;/p&gt;
&lt;p&gt;Let's get set up to make shifts automatically.
Since they're aren't many dependencies all the way down, start by getting phantomjs installed, then the phantom-crowbar.&lt;/p&gt;
&lt;h2&gt;Installing phantom-crowbar&lt;/h2&gt;
&lt;p&gt;For the phantomjs, I prefer to use homebrew:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew update
brew upgrade
brew install phantomjs
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Then to get the crowbar, clone the git repository.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; ~
git clone https://github.com/andyreagan/phantom-crowbar
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;To use it system-wide, I use the bash alias:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;alias&lt;/span&gt; phantom-crowbar&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/usr/local/bin/phantomjs ~/phantom-crowbar/phantom-crowbar.js&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Without too much detail, I recommend adding this to your &lt;code&gt;~/.bash_profile&lt;/code&gt; so that it's loaded every time you start a terminal session.&lt;/p&gt;
&lt;h2&gt;Installing inkscape&lt;/h2&gt;
&lt;p&gt;You only need inkscape if you want to go from svg to pdf (and there are other ways too), but this one is easy with, again, homebrew.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install inkscape
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Installing labMTsimple&lt;/h2&gt;
&lt;p&gt;There are two ways to get it: using pip of cloning the git repo.
If you're not sure, use pip.
I think pip makes it easier to keep it up to date, etc.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install labMTsimple
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Making your first shift&lt;/h2&gt;
&lt;p&gt;If you cloned the git repository, install the thing and then you can check out the example in &lt;code&gt;examples/example.py&lt;/code&gt;.
If you went with pip, see that file on &lt;a href="https://github.com/andyreagan/labMT-simple/blob/master/examples/example.py"&gt;github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The basic idea is to generate the word vectors, and then pass them to &lt;code&gt;shiftHtml()&lt;/code&gt;.
In a little bit more detail, the key pieces are:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# load the module&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;labMTsimple.storyLab&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;

&lt;span class="c1"&gt;# get word, and word score frequency vectors&lt;/span&gt;
&lt;span class="n"&gt;labMT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labMTvector&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labMTwordList&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emotionFileReader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;stopval&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;returnVector&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# see the example file, but assuming there are two strings to score&lt;/span&gt;
&lt;span class="c1"&gt;# named &amp;quot;saturday&amp;quot; and &amp;quot;tuesday&amp;quot;, generate their word frequency vectors&lt;/span&gt;
&lt;span class="n"&gt;saturdayValence&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;saturdayFvec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emotion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;saturday&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labMT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;shift&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;happsList&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labMTvector&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tuesdayValence&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tuesdayFvec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;emotion&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tuesday&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labMT&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;shift&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="bp"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;happsList&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;labMTvector&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# apply the traditional lens to the frequency vectors&lt;/span&gt;
&lt;span class="c1"&gt;# this sets the frequency of words with score between&lt;/span&gt;
&lt;span class="c1"&gt;# 4 and 6 (corresponding to stopval of 1)&lt;/span&gt;
&lt;span class="n"&gt;tuesdayStoppedVec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stopper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tuesdayFvec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labMTvector&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labMTwordList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;stopVal&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;saturdayStoppedVec&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stopper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;saturdayFvec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labMTvector&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labMTwordList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;stopVal&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# generate an html file&lt;/span&gt;
&lt;span class="c1"&gt;# and make a static directory&lt;/span&gt;
&lt;span class="n"&gt;shiftHtml&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;labMTvector&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;labMTwordList&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;tuesdayStoppedVec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="n"&gt;saturdayStoppedVec&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;wordshift.html&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That last call actually does quite a few things, in detail:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Creates a directory called &lt;code&gt;static&lt;/code&gt;, in your working directory&lt;/li&gt;
&lt;li&gt;Copies in a handful of files into &lt;code&gt;static&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Writes a javascript file called &lt;code&gt;static/wordshift.js&lt;/code&gt; that has the lens, words, and both frequency vectors written out. The name is based off the root of the html file you've told it to write.&lt;/li&gt;
&lt;li&gt;Writes an html file, which loads all the javascript in static.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Phew! We've made the html file. You can open it directly in Chrome (or your browser of choice...IE not tested).&lt;/p&gt;
&lt;p&gt;The optional, final step is to make the svg and/or pdf:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;phantom-crowbar wordshift.html shiftsvg wordshift.svg
inkscape -f wordshift.svg -A wordshift.pdf
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, check out wordshift.pdf for the file!
And I'm working on upgrades, like optionally removing the shift button or putting numbers on the axes, so look out for more.&lt;/p&gt;
&lt;p&gt;And feel free to tweet suggestions at &lt;a href="https://twitter.com/andyreagan"&gt;@andyreagan&lt;/a&gt;, and submit pull requests to the &lt;a href="https://github.com/andyreagan/labMT-simple"&gt;source code&lt;/a&gt;!&lt;/p&gt;</content></entry><entry><title>Announcing the Phantom Crowbar</title><link href="/2014/09/19/announcing-the-phantom-crowbar/" rel="alternate"></link><published>2014-09-19T00:00:00+02:00</published><updated>2014-09-19T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2014-09-19:/2014/09/19/announcing-the-phantom-crowbar/</id><summary type="html">&lt;p&gt;Extract the raw SVG (and styles) from a webpage, from the command line!&lt;/p&gt;
&lt;p&gt;Motivated by the desire to use d3 to make figures automatically.&lt;/p&gt;
&lt;p&gt;Built on top of the wonderful, and more versatile, &lt;a href="http://nytimes.github.io/svg-crowbar/"&gt;svg-crowbar&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I'm testing it on hedonometer.org, and remaining issues are converting to a PDF correctly. But …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Extract the raw SVG (and styles) from a webpage, from the command line!&lt;/p&gt;
&lt;p&gt;Motivated by the desire to use d3 to make figures automatically.&lt;/p&gt;
&lt;p&gt;Built on top of the wonderful, and more versatile, &lt;a href="http://nytimes.github.io/svg-crowbar/"&gt;svg-crowbar&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I'm testing it on hedonometer.org, and remaining issues are converting to a PDF correctly. But this is likely a result of haphazard use of CSS styles in the page design.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dependencies&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;phantomjs&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install phantomjs
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Usage&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Clone this repository.&lt;/p&gt;
&lt;p&gt;Run the &lt;code&gt;phantom-crowbar.js&lt;/code&gt; with &lt;code&gt;phantomjs&lt;/code&gt;, passing the webpage, the id of the svg element to grab, and the output filename.
Here's the first line of test/run.sh:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;phantomjs phantom-crowbar.js http://hedonometer.org timeseries test1.svg
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Uses&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;inkscape&lt;/li&gt;
&lt;li&gt;CairoSVG&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install inkscape
pip install CairoSVG
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Run tests with &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="nb"&gt;test&lt;/span&gt;
. run.sh
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Inkscape will throw a lot of warnings.
As mentioned above, there are still plenty of kinks to work out.&lt;/p&gt;</content></entry><entry><title>Github pages with Pelican</title><link href="/2014/09/04/github-pages-with-pelican/" rel="alternate"></link><published>2014-09-04T00:00:00+02:00</published><updated>2014-09-04T00:00:00+02:00</updated><author><name>andy reagan</name></author><id>tag:None,2014-09-04:/2014/09/04/github-pages-with-pelican/</id><summary type="html">&lt;p&gt;If I get this on github pages, it will be a miracle.&lt;/p&gt;
&lt;p&gt;So, I did. Here's how.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Note the date on this post, things may have changed.&lt;/li&gt;
&lt;li&gt;Be ready with homebrew (on your Mac) and a github account.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Install virtualenv (with pip).&lt;/p&gt;
&lt;p&gt;Create a new directory, i.e. "website", and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If I get this on github pages, it will be a miracle.&lt;/p&gt;
&lt;p&gt;So, I did. Here's how.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Note the date on this post, things may have changed.&lt;/li&gt;
&lt;li&gt;Be ready with homebrew (on your Mac) and a github account.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Install virtualenv (with pip).&lt;/p&gt;
&lt;p&gt;Create a new directory, i.e. "website", and start a virtual environment.
I use autoenv (brew install autoenv), which executes a .env file if it exists when you change into a directory.
Autoenv will ask you to confirm the .env file is correct.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;cd&lt;/span&gt; website
virtualenv pyenv
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;source /Users/andyreagan/website/pyenv/bin/activate&amp;quot;&lt;/span&gt; &amp;gt; .env
&lt;span class="nb"&gt;cd&lt;/span&gt; .
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now your prompt should be prefixed by "(pyenv)". Now start installing pelican.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pip install pelican
pip install markdown
pip install ghp-import
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you're going to migrate over posts from somewhere else (wordpress for me), get pandoc.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;brew install pandoc
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now you can get started building the website with Pelican. First, make sure the github is set up. Follow the instructions on https://pages.github.com/, and leave the repository empty.
Clone it into your working directory (website) with&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git clone https://github.com/andyreagan/andyreagan.github.io .
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Finally, build the website with Pelican. I just followed the tutorial on their documentation, which is excellent: http://docs.getpelican.com/en/3.4.0/quickstart.html&lt;/p&gt;
&lt;p&gt;Once you've got something showing in localhost, it's time to push it up to github. Hold your breath.
The publishing script that I use, very simple is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# publishsimple.sh&lt;/span&gt;

pelican content -s pelicanconf.py 
ghp-import output
git push -f origin gh-pages:master
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Some other goodies. All of this is well documented within Pelican's docs, but here it is. This is the tail of my pelicanconf.py:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# static paths will be copied without parsing their contents&lt;/span&gt;
&lt;span class="n"&gt;STATIC_PATHS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;images&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;files&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# get a homepage on the menu bar&lt;/span&gt;
&lt;span class="n"&gt;MENUITEMS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Home&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/index.html&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,),]&lt;/span&gt;

&lt;span class="c1"&gt;# make the URL&amp;#39;s look nice&lt;/span&gt;
&lt;span class="n"&gt;ARTICLE_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{date:%Y}/{date:%m}/{date:&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;}/{slug}/&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;ARTICLE_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{date:%Y}/{date:%m}/{date:&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="s1"&gt;}/{slug}/index.html&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;YEAR_ARCHIVE_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{date:%Y}/index.html&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;MONTH_ARCHIVE_SAVE_AS&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;{date:%Y}/{date:%b}/index.html&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And I did fork my own theme. Copied over the theme from "pyenv/lib/python2.7/site-packages/pelican/themes/simple/templates/" into "theme/finnigan".
I also created the "static" folder alongside "templates," and put the "css" and "js" and "images" folders inside static.&lt;/p&gt;
&lt;p&gt;Then set this to be the theme in "pelicanconf.sh":&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;THEME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;theme/finnigan&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;If you've made it this far, start writing!&lt;/p&gt;
&lt;p&gt;Cheers!&lt;/p&gt;</content></entry></feed>